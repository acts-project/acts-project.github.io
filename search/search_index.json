{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A Common Tracking Software (Acts) Introduction Repository structure Releases License and authors Latest release: v0.22.00 16 Apr 2020 Download Release Notes Documentation Introduction This project contains an experiment-independent set of track reconstruction tools. The main philosophy is to provide high-level track reconstruction modules that can be used for any tracking detector. The description of the tracking detector's geometry is optimized for efficient navigation and quick extrapolation of tracks. Converters for several common geometry description languages exist. Having a highly performant, yet largely customizable implementation of track reconstruction algorithms was a primary objective for the design of this toolset. Additionally, the applicability to real-life HEP experiments plays major role in the development process. Apart from algorithmic code, this project also provides an event data model for the description of track parameters and measurements. Key features of this project include: tracking geometry description which can be constructed from TGeo, DD4Hep, or GDML input, simple and efficient event data model, performant and highly flexible algorithms for track propagation and fitting, basic seed finding algorithms. Mailing list In order to receive the latest updates, users of the Acts project are encouraged to subscribe to acts-users@cern.ch . This list provides: regular updates on the software, access to the Acts JIRA project for bug fixes/feature requests, a common place for asking any kind of questions. Repository structure The repositories for the Acts project can be found at https://gitlab.cern.ch/acts acts-core The acts-core repository contains the all detector independent software that is part of the Acts toolkit. acts-fatras The acts-fatras repository contains the fast track simulation extension of the Acts toolkit. The fast track simulation moduels are currently in development and available only in a limited way. acts-data The acts-data repository contains some necessary data files for running the standard Acts examples. acts-framework The acts-framework reposity contains a small event processing framework for development and testing. The framework is capable of parallel event processing if OpenMP is available on the machine. Acts test jobs are run in the framework in the continuous integration, and tested for bit-wise identical result between single and multithreaded mode. The acts-framework includes acts-core and acts-fatras as submodules in an external folder. Releases History release time links v0.22.00 16 Apr 2020 10:21 Release Notes , Download , Documentation v0.20.00 27 Mar 2020 19:01 Release Notes , Download , Documentation v0.19.00 23 Mar 2020 14:00 Release Notes , Download , Documentation v0.18.00 13 Mar 2020 18:43 Release Notes , Download , Documentation v0.17.00 06 Mar 2020 18:23 Release Notes , Download , Documentation v0.16.00 05 Feb 2020 17:33 Release Notes , Download , Documentation v0.15.00 24 Jan 2020 13:17 Release Notes , Download , Documentation v0.14.00 20 Jan 2020 14:16 Release Notes , Download , Documentation v0.13.00 19 Dec 2019 16:59 Release Notes , Download , Documentation v0.12.01 19 Dec 2019 15:16 Release Notes , Download , Documentation v0.12.00 10 Dec 2019 12:52 Release Notes , Download , Documentation v0.11.01 26 Nov 2019 14:21 Release Notes , Download , Documentation v0.11.00 22 Nov 2019 09:30 Release Notes , Download , Documentation v0.10.05 21 Oct 2019 09:47 Release Notes , Download , Documentation v0.10.04 03 Sep 2019 11:43 Release Notes , Download , Documentation v0.10.03 17 Jul 2019 17:00 Release Notes , Download , Documentation v0.09.05 16 Jul 2019 14:55 Release Notes , Download , Documentation v0.09.04 15 Jul 2019 12:38 Release Notes , Download , Documentation v0.10.02 27 Jun 2019 14:23 Release Notes , Download , Documentation v0.09.03 27 Jun 2019 14:15 Release Notes , Download , Documentation v0.10.01 29 May 2019 17:33 Release Notes , Download , Documentation v0.10.00 29 May 2019 14:46 Release Notes , Download , Documentation v0.09.02 03 May 2019 08:59 Release Notes , Download , Documentation v0.09.01 29 Apr 2019 10:26 Release Notes , Download , Documentation v0.09.00 17 Apr 2019 11:22 Release Notes , Download , Documentation v0.08.02 21 Mar 2019 12:37 Release Notes , Download , Documentation v0.08.01 18 Feb 2019 13:28 Release Notes , Download , Documentation v0.08.00 19 Dec 2018 18:33 Release Notes , Download , Documentation v0.07.03 12 Dec 2018 09:01 Release Notes , Download , Documentation v0.07.02 06 Nov 2018 10:33 Release Notes , Download , Documentation v0.07.01 02 Nov 2018 16:20 Release Notes , Download , Documentation v0.07.00 03 Sep 2018 15:32 Release Notes , Download , Documentation v0.06.00 27 Apr 2018 16:43 Release Notes , Download , Documentation v0.05.03 13 Dec 2017 16:35 Release Notes , Download , Documentation v0.05.02 25 Oct 2017 16:21 Release Notes , Download , Documentation v0.05.01 04 Oct 2017 18:13 Release Notes , Download , Documentation v0.05.00 31 Aug 2017 15:12 Release Notes , Download , Documentation v0.04.01 30 May 2017 14:16 Release Notes , Download , Documentation v0.04.00 09 May 2017 13:40 Release Notes , Download , Documentation v0.04.99 24 Mar 2017 11:36 Release Notes , Download , Documentation v0.03.02 24 Mar 2017 12:41 Release Notes , Download , Documentation v0.03.01 28 Nov 2016 14:22 Release Notes , Download , Documentation v0.03.00 11 Nov 2016 17:18 Release Notes , Download , Documentation v0.02.01 26 Sep 2016 14:12 Release Notes , Download , Documentation v0.02.00 18 Sep 2016 00:43 Release Notes , Download , Documentation v0.01.01 13 Jun 2016 10:54 Release Notes , Download , Documentation v0.01.00 09 Jun 2016 13:31 Release Notes , Download , Documentation License and authors This project is published under the Mozilla Public License, v. 2.0. Details of this license can be found in the LICENSE file or at http://mozilla.org/MPL/2.0/ . Contributors to the Acts project are listed in here . The Acts project is based on the ATLAS tracking software. A list of contributors to the ATLAS tracking repository can be found here . The Acts project contains a copy of gcovr licensed under the 3-Clause BSD license.","title":"Home"},{"location":"#a-common-tracking-software-acts","text":"Introduction Repository structure Releases License and authors","title":"A Common Tracking Software (Acts)"},{"location":"#latest-release-v02200","text":"16 Apr 2020 Download Release Notes Documentation","title":"Latest release: v0.22.00"},{"location":"#introduction","text":"This project contains an experiment-independent set of track reconstruction tools. The main philosophy is to provide high-level track reconstruction modules that can be used for any tracking detector. The description of the tracking detector's geometry is optimized for efficient navigation and quick extrapolation of tracks. Converters for several common geometry description languages exist. Having a highly performant, yet largely customizable implementation of track reconstruction algorithms was a primary objective for the design of this toolset. Additionally, the applicability to real-life HEP experiments plays major role in the development process. Apart from algorithmic code, this project also provides an event data model for the description of track parameters and measurements. Key features of this project include: tracking geometry description which can be constructed from TGeo, DD4Hep, or GDML input, simple and efficient event data model, performant and highly flexible algorithms for track propagation and fitting, basic seed finding algorithms.","title":"Introduction"},{"location":"#mailing-list","text":"In order to receive the latest updates, users of the Acts project are encouraged to subscribe to acts-users@cern.ch . This list provides: regular updates on the software, access to the Acts JIRA project for bug fixes/feature requests, a common place for asking any kind of questions.","title":"Mailing list"},{"location":"#repository-structure","text":"The repositories for the Acts project can be found at https://gitlab.cern.ch/acts","title":"Repository structure"},{"location":"#acts-core","text":"The acts-core repository contains the all detector independent software that is part of the Acts toolkit.","title":"acts-core"},{"location":"#acts-fatras","text":"The acts-fatras repository contains the fast track simulation extension of the Acts toolkit. The fast track simulation moduels are currently in development and available only in a limited way.","title":"acts-fatras"},{"location":"#acts-data","text":"The acts-data repository contains some necessary data files for running the standard Acts examples.","title":"acts-data"},{"location":"#acts-framework","text":"The acts-framework reposity contains a small event processing framework for development and testing. The framework is capable of parallel event processing if OpenMP is available on the machine. Acts test jobs are run in the framework in the continuous integration, and tested for bit-wise identical result between single and multithreaded mode. The acts-framework includes acts-core and acts-fatras as submodules in an external folder.","title":"acts-framework"},{"location":"#releases","text":"","title":"Releases"},{"location":"#history","text":"release time links v0.22.00 16 Apr 2020 10:21 Release Notes , Download , Documentation v0.20.00 27 Mar 2020 19:01 Release Notes , Download , Documentation v0.19.00 23 Mar 2020 14:00 Release Notes , Download , Documentation v0.18.00 13 Mar 2020 18:43 Release Notes , Download , Documentation v0.17.00 06 Mar 2020 18:23 Release Notes , Download , Documentation v0.16.00 05 Feb 2020 17:33 Release Notes , Download , Documentation v0.15.00 24 Jan 2020 13:17 Release Notes , Download , Documentation v0.14.00 20 Jan 2020 14:16 Release Notes , Download , Documentation v0.13.00 19 Dec 2019 16:59 Release Notes , Download , Documentation v0.12.01 19 Dec 2019 15:16 Release Notes , Download , Documentation v0.12.00 10 Dec 2019 12:52 Release Notes , Download , Documentation v0.11.01 26 Nov 2019 14:21 Release Notes , Download , Documentation v0.11.00 22 Nov 2019 09:30 Release Notes , Download , Documentation v0.10.05 21 Oct 2019 09:47 Release Notes , Download , Documentation v0.10.04 03 Sep 2019 11:43 Release Notes , Download , Documentation v0.10.03 17 Jul 2019 17:00 Release Notes , Download , Documentation v0.09.05 16 Jul 2019 14:55 Release Notes , Download , Documentation v0.09.04 15 Jul 2019 12:38 Release Notes , Download , Documentation v0.10.02 27 Jun 2019 14:23 Release Notes , Download , Documentation v0.09.03 27 Jun 2019 14:15 Release Notes , Download , Documentation v0.10.01 29 May 2019 17:33 Release Notes , Download , Documentation v0.10.00 29 May 2019 14:46 Release Notes , Download , Documentation v0.09.02 03 May 2019 08:59 Release Notes , Download , Documentation v0.09.01 29 Apr 2019 10:26 Release Notes , Download , Documentation v0.09.00 17 Apr 2019 11:22 Release Notes , Download , Documentation v0.08.02 21 Mar 2019 12:37 Release Notes , Download , Documentation v0.08.01 18 Feb 2019 13:28 Release Notes , Download , Documentation v0.08.00 19 Dec 2018 18:33 Release Notes , Download , Documentation v0.07.03 12 Dec 2018 09:01 Release Notes , Download , Documentation v0.07.02 06 Nov 2018 10:33 Release Notes , Download , Documentation v0.07.01 02 Nov 2018 16:20 Release Notes , Download , Documentation v0.07.00 03 Sep 2018 15:32 Release Notes , Download , Documentation v0.06.00 27 Apr 2018 16:43 Release Notes , Download , Documentation v0.05.03 13 Dec 2017 16:35 Release Notes , Download , Documentation v0.05.02 25 Oct 2017 16:21 Release Notes , Download , Documentation v0.05.01 04 Oct 2017 18:13 Release Notes , Download , Documentation v0.05.00 31 Aug 2017 15:12 Release Notes , Download , Documentation v0.04.01 30 May 2017 14:16 Release Notes , Download , Documentation v0.04.00 09 May 2017 13:40 Release Notes , Download , Documentation v0.04.99 24 Mar 2017 11:36 Release Notes , Download , Documentation v0.03.02 24 Mar 2017 12:41 Release Notes , Download , Documentation v0.03.01 28 Nov 2016 14:22 Release Notes , Download , Documentation v0.03.00 11 Nov 2016 17:18 Release Notes , Download , Documentation v0.02.01 26 Sep 2016 14:12 Release Notes , Download , Documentation v0.02.00 18 Sep 2016 00:43 Release Notes , Download , Documentation v0.01.01 13 Jun 2016 10:54 Release Notes , Download , Documentation v0.01.00 09 Jun 2016 13:31 Release Notes , Download , Documentation","title":"History"},{"location":"#license-and-authors","text":"This project is published under the Mozilla Public License, v. 2.0. Details of this license can be found in the LICENSE file or at http://mozilla.org/MPL/2.0/ . Contributors to the Acts project are listed in here . The Acts project is based on the ATLAS tracking software. A list of contributors to the ATLAS tracking repository can be found here . The Acts project contains a copy of gcovr licensed under the 3-Clause BSD license.","title":"License and authors"},{"location":"authors/","text":"Authors Contributors to Acts in alphabetical order: Andreas Salzburger, CERN Bastian Schlag, CERN, JGU Mainz Christian Gumpert Fabian Klimpel, Max-Planck-Institut f\u00fcr Physik Hadrien Grasland, Centre National de la Recherche Scientifique Julia Hrdinka Moritz Kiehn, Universit\u00e9 de Gen\u00e8ve Noemi Calace, CERN Paul Gessinger, CERN, JGU Mainz Robert Langenberg, CERN Xiaocong Ai, LBNL ( Detailed contributor information ) Contributors to the ATLAS tracking software The Acts development team is grateful for the work done by the following people on the ATLAS tracking software. Alejandro Alonso Diaz, Adam Edward Barton, Andy Buckley, Anthony Morley, Aaron James Armbruster, Olivier Arnaez, John Baines, Sebastien Binet, Bruno Lenzi, Veronique Boisvert, Guennadi Borissov, Eva Bouhova-Thacker, Antonio Boveia, Christian Gumpert, Christian Schmitt, Camilla Maiani, Andrea Coccaro, Christian Ohm, Christoph Rauchegger, Christoph Wasicki, Daniel Ryan Blackburn, Dave Casper, David Divalentino, Dmitry Emeliyanov, Daniel Hay Guest, Daniel Kollar, David Lopez Mateos, Daniel Mori, David Quarrie, David Rousseau, David Richard Shope, Till Eifert, Esben Lund, Edward Moyse, Frederic Brochu, Sebastian Fleischmann, Fred Luehring, Felix Socher, Igor Gavrilenko, Gerhard Immanue Brandt, Graham John Cree, Peter Van Gemmeren, Giacinto Piacquadio, Goetz Gaycken, Steven Goldfarb, Grant Gorfine, Carl Bryan Gwilliam, Heberth Jesus Torres Davila, Loek Hooft Van Huysduynen, Haichen Wang, Ioannis Nomidis, Ilija Vukotic, Jahred Adelman, Javier Jimenez Pena, Johanna Bronner, James Catmore, John Derek Chapman, Jason Lee, Jeremy Robert Love, John Alison, Jochen Meyer, Juergen Thomas, Jochem Snuverink, Jeffrey Tseng, Jike Wang, Konstantinos Karakostas, Kathryn Grimm, Peter Kluit, Thomas Koffas, Nikolaos Konstantinidis, Attila Krasznahorkay, Vicente Lacuesta Miquel, Remi Lafaye, Antonio Limosani, Markus Elsing, Salvador Marti I Garcia, Jiri Masik, Massimiliano Bellomo, Matthias Danninger, Miriam Deborah Diamond, Joerg Mechnich, Maria Jose Costa Mezquita, Maaike Limper, Manuel Neumann, Marcin Nowak, Matthew Scott Rudolph, Maximiliano Sioli, Michael Ughetto, Noemi Calace, Nectarios Benekos, Niels Van Eldik, Nir Amram, Nathan Rogers Bernard, Nicholas Styles, Konstantinos Ntekas, Emil Obreshkov, Susumu Oda, Gustavo Ordonez Sanz, Ourania Sidiropoulou, Petr Balek, Pawel Bruckman De Renstrom, Pierfrancesco Butti, Troels Petersen, Kirill Prokofiev, Alan Poppleton, Rachel Reisner Hinman, Elmar Ritsch, Roland Jansky, Markus Jungst, Robert Johannes Langenberg, Robert Harrington, Ruslan Mashinistov, Andreas Salzburger, Sebastian Boeser, R D Schaffer, Shih-Chieh Hsu, Sabine Elles, Rolf Seuster, Silvia Miglioranzi, Stewart Martin-Haugh, Song-Ming Wang, Simone Pagan Griso, Shaun Roe, Savanna Marie Shaw, Scott Snyder, Stefania Spagnolo, Thijs Cornelissen, Tommaso Lari, Tatjana Lenz, Sarka Todorova, Vakhtang Tsulaia, Vato Kartvelishvili, Veerle Heijne, Vadim Kostyukhin, Weimin Song, Andreas Wildauer, Walter Lampl, William Axel Leight, Wolfgang Liebig, Wolfgang Lukas, Marcin Wolter","title":"Authors"},{"location":"authors/#authors","text":"","title":"Authors"},{"location":"authors/#contributors-to-acts-in-alphabetical-order","text":"Andreas Salzburger, CERN Bastian Schlag, CERN, JGU Mainz Christian Gumpert Fabian Klimpel, Max-Planck-Institut f\u00fcr Physik Hadrien Grasland, Centre National de la Recherche Scientifique Julia Hrdinka Moritz Kiehn, Universit\u00e9 de Gen\u00e8ve Noemi Calace, CERN Paul Gessinger, CERN, JGU Mainz Robert Langenberg, CERN Xiaocong Ai, LBNL ( Detailed contributor information )","title":"Contributors to Acts in alphabetical order:"},{"location":"authors/#contributors-to-the-atlas-tracking-software","text":"The Acts development team is grateful for the work done by the following people on the ATLAS tracking software. Alejandro Alonso Diaz, Adam Edward Barton, Andy Buckley, Anthony Morley, Aaron James Armbruster, Olivier Arnaez, John Baines, Sebastien Binet, Bruno Lenzi, Veronique Boisvert, Guennadi Borissov, Eva Bouhova-Thacker, Antonio Boveia, Christian Gumpert, Christian Schmitt, Camilla Maiani, Andrea Coccaro, Christian Ohm, Christoph Rauchegger, Christoph Wasicki, Daniel Ryan Blackburn, Dave Casper, David Divalentino, Dmitry Emeliyanov, Daniel Hay Guest, Daniel Kollar, David Lopez Mateos, Daniel Mori, David Quarrie, David Rousseau, David Richard Shope, Till Eifert, Esben Lund, Edward Moyse, Frederic Brochu, Sebastian Fleischmann, Fred Luehring, Felix Socher, Igor Gavrilenko, Gerhard Immanue Brandt, Graham John Cree, Peter Van Gemmeren, Giacinto Piacquadio, Goetz Gaycken, Steven Goldfarb, Grant Gorfine, Carl Bryan Gwilliam, Heberth Jesus Torres Davila, Loek Hooft Van Huysduynen, Haichen Wang, Ioannis Nomidis, Ilija Vukotic, Jahred Adelman, Javier Jimenez Pena, Johanna Bronner, James Catmore, John Derek Chapman, Jason Lee, Jeremy Robert Love, John Alison, Jochen Meyer, Juergen Thomas, Jochem Snuverink, Jeffrey Tseng, Jike Wang, Konstantinos Karakostas, Kathryn Grimm, Peter Kluit, Thomas Koffas, Nikolaos Konstantinidis, Attila Krasznahorkay, Vicente Lacuesta Miquel, Remi Lafaye, Antonio Limosani, Markus Elsing, Salvador Marti I Garcia, Jiri Masik, Massimiliano Bellomo, Matthias Danninger, Miriam Deborah Diamond, Joerg Mechnich, Maria Jose Costa Mezquita, Maaike Limper, Manuel Neumann, Marcin Nowak, Matthew Scott Rudolph, Maximiliano Sioli, Michael Ughetto, Noemi Calace, Nectarios Benekos, Niels Van Eldik, Nir Amram, Nathan Rogers Bernard, Nicholas Styles, Konstantinos Ntekas, Emil Obreshkov, Susumu Oda, Gustavo Ordonez Sanz, Ourania Sidiropoulou, Petr Balek, Pawel Bruckman De Renstrom, Pierfrancesco Butti, Troels Petersen, Kirill Prokofiev, Alan Poppleton, Rachel Reisner Hinman, Elmar Ritsch, Roland Jansky, Markus Jungst, Robert Johannes Langenberg, Robert Harrington, Ruslan Mashinistov, Andreas Salzburger, Sebastian Boeser, R D Schaffer, Shih-Chieh Hsu, Sabine Elles, Rolf Seuster, Silvia Miglioranzi, Stewart Martin-Haugh, Song-Ming Wang, Simone Pagan Griso, Shaun Roe, Savanna Marie Shaw, Scott Snyder, Stefania Spagnolo, Thijs Cornelissen, Tommaso Lari, Tatjana Lenz, Sarka Todorova, Vakhtang Tsulaia, Vato Kartvelishvili, Veerle Heijne, Vadim Kostyukhin, Weimin Song, Andreas Wildauer, Walter Lampl, William Axel Leight, Wolfgang Liebig, Wolfgang Lukas, Marcin Wolter","title":"Contributors to the ATLAS tracking software"},{"location":"guides_admin/","text":"Administrator's corner This section gives useful information to the administrators of the Acts project. For normal developers the sections below are irrelevant. Make a new Acts release In order to release a new version of Acts the following steps need to be taken: Check out all open issues and MRs associated with the milestone that you want to tag here . Merge master into the release branch. In that branch, change the content of the file version_number at the repository root to the new version X.Y.Z and commit. Pushing this commit to the remote repository should trigger a CI build. Make sure that everything compiles without any warnings and all tests look fine. Create a new annotated tag locally. The tag should have the format vX.YY.ZZ and an associated tag message 'version vX.YY.ZZ' and should point to the commit created in step 3. Push the tag to the remote repository. This should trigger a CI job which rebuilds to documentation and deploys it to the Acts webpage. Make sure that the new release appears under the Releases section on the Acts webpage . If there is not yet a milestone for the next release, create it in Gitlab (e.g. if 1.23.02 was just released, a milestone version 1.23.03 should exist for the next minor release for bug fixes). Check that the release notes appear on the release's page on GitLab under: https://gitlab.cern.ch/acts/acts-core/tags/vX.YY.ZZ after the post-merge CI jobs have completed.","title":"Administrators Guide"},{"location":"guides_admin/#administrators-corner","text":"This section gives useful information to the administrators of the Acts project. For normal developers the sections below are irrelevant.","title":"Administrator's corner"},{"location":"guides_admin/#make-a-new-acts-release","text":"In order to release a new version of Acts the following steps need to be taken: Check out all open issues and MRs associated with the milestone that you want to tag here . Merge master into the release branch. In that branch, change the content of the file version_number at the repository root to the new version X.Y.Z and commit. Pushing this commit to the remote repository should trigger a CI build. Make sure that everything compiles without any warnings and all tests look fine. Create a new annotated tag locally. The tag should have the format vX.YY.ZZ and an associated tag message 'version vX.YY.ZZ' and should point to the commit created in step 3. Push the tag to the remote repository. This should trigger a CI job which rebuilds to documentation and deploys it to the Acts webpage. Make sure that the new release appears under the Releases section on the Acts webpage . If there is not yet a milestone for the next release, create it in Gitlab (e.g. if 1.23.02 was just released, a milestone version 1.23.03 should exist for the next minor release for bug fixes). Check that the release notes appear on the release's page on GitLab under: https://gitlab.cern.ch/acts/acts-core/tags/vX.YY.ZZ after the post-merge CI jobs have completed.","title":"Make a new Acts release"},{"location":"guides_contribution/","text":"Contributing to Acts Contributions to the Acts project are very welcome and feedback on the documentation is greatly appreciated. In order to be able to contribute to the Acts project, developers must have a valid CERN user account. Unfortunately, lightweight CERN accounts for external users do not have sufficient permissions to access certain CERN services used by the Acts project. Mailing lists Bug reports and feature requests Make a contribution Preparation New development Creating a merge request Using forks Workflow recommendations Coding style and guidelines git tips and tricks Review other contributions Approving a merge request Merging a merge request Administrator's corner Making a new Acts release Mailing lists acts-users@cern.ch : Users of the Acts project should subscribe to this list as it provides: regular updates on the software, access to the Acts JIRA project for bug fixes/feature requests, a common place for asking any kind of questions. acts-developers@cern.ch : Developers are encouraged to also subscribe to this list as it provides you with: a developer role in the Acts JIRA project (allows you to handle tickets), developer access to the Acts git repository information about developer meetings, a common place for technical discussions. Bug reports and feature requests If you want to report or start a feature request, please open a ticket in the Acts JIRA ( Note: access is restricted to members of the mailing lists mentioned above). A comprehensive explanation will help the development team to respond in a timely manner. Therefore, the following details should be mentioned: bug reports issue type: \"Bug\" summary: short description of the problem priority: will be set by the development team components: if known, part of Acts affected by this bug; leave empty otherwise affects version: version of Acts affected by this bug a detailed description of the bug including a receipe on how to reproduce it and any hints which may help diagnosing the problem feature requests issue type: \"Improvement\" or \"New Feature\" summary: short description of new feature priority: will be set by the development team a detailed description of the feature request including possible use cases and benefits for other users Make a contribution The instructions below should help you getting started with development process in the Acts project. If you have any questions, feel free to ask acts-developers@cern for help or guidance. The Acts project uses a git repository which is hosted on the CERN GitLab server. In order to be able to push your changes and to create merge requests (and thus, contribute to the development of Acts), you must be subscribed to acts-developers@cern.ch. A general introduction to the GitLab web interface can be found here . Very nice tutorials as well as explanations of concepts and workflows with git can be found on Atlassian . For a shorter introduction and the full git documentation have a look at the git tutorial . Preparation Configuring git Commits to repositories on the CERN GitLab server are only accepted from CERN users. Therefore, it is important that git is correctly configured on all machines you are working on. It is necessary to that the git user email address to the primary email address of your CERN account (usually: firstname.lastname@cern.ch). You can check the current values with: git config user.name git config user.email You can change those settings by either editing the .gitconfig file in your home directory or by running: git config user.name git config --global user.name \"Donald Duck\" git config --global user.email \"donald.duck@cern.ch\" Further recommended settings are: git config user.name git config --global push.default simple git config --global pull.rebase true Getting a local working copy As a first step, you need to clone the Acts repository which gives you a local, fully functional and self-contained git repository. This means you can work locally (commit changes, compare versions, inspect the history) while being offline. An internet connection is only required for pull ing in new updates or push ing your changes to the Gitlab server. git clone <ACTS_URL> <DESTINATION> <ACTS_URL> can be found on the project page of the Acts git repository . There are different URLs for different authentication methods. <DESTINATION> is optional and gives the path on your local machine where the clone will be created. Starting a new development When you start a new development, you should make sure to start from the most recent version. Therefore, you need to fetch the latest changes from the remote repository by running from inside the Acts directory: git fetch origin Now you can create a new branch for your development using git checkout -b <BRANCH_NAME> <UPSTREAM> --no-track Let's go through the options one by one: -b <BRANCH_NAME> creates a new branch called <BRANCH_NAME>. <UPSTREAM> specifies the starting point of the new branch. For feature developments, this should be origin/master , for bug fixes in older releases it could also be for instance origin/release-0.03.X . --no-track decouples your local branch from the remote branch. This is helpful because you do not have the permissions to push any commits directly into the remote master / release-X,Y,Z branch. Creating a merge request Once your development is ready for integration, you should open a merge request at the Acts project ( GitLab Help: Create a merge request ). The target branch should usually be master for feature requests and releas-X,Y,Z for bugfixes. The Acts projects accepts only fast-foward merges which means that your branch must have been rebased on the target branch. This can be achieved by fetching upstream changes and rebasing afterwards: git fetch origin git checkout <my_feature_branch> git rebase -i origin/<target_branch> git push -u origin <my_feature_branch> At this point you should make use of the interactive rebase procedure to clean up your commits (squash small fixes, improve commit messages etc; Rewriting history ). Merge requests are required to close a Acts JIRA ticket. This is achieved by adding e.g. 'fixes ACTS-XYZ' to the end of the merge request description. Please note that JIRA tickets should only be referenced by merge requests and not individual commits (since strictly there should only be one JIRA ticket per merge request). Once the merge request is opened, a continous integration job is triggered which will add multiple comments to the merge request (e.g. build status, missing license statements, doxygen errors, test coverage etc). Please have a look at them and fix them by adding more commits to your branch if needed. Please find below a short checklist for merge requests. Using forks Users or groups with git experience may prefer to use their own forks of the Acts project to develop and test their changes before they are integrated back into the main Acts repository. We assume that you are experienced with a fork-based workflow in git and only summarise the most important commands at the end of this section. As a first step, you need to create your own fork of the Acts project. For doing this, please go to the Acts GitLab page , click on the fork button, and follow the instructions ( GitLab Help \"How to fork a project\" ). To enable the CI on your fork, go from your project page to \"Settings -> CI / CD\", expand the \"Runners\" menu and click on \"Enable shared Runners\" for this project. # adding the central Acts repository as remote git remote add upstream ssh://git@gitlab.cern.ch:7999/acts/acts-core.git # get latest upstream changes git fetch upstream # start a new development git checkout -b <BRANCH_NAME> upstream/master --no-track # code, test, document # publish your changes to your fork git push -u origin <BRANCH_NAME> # open a merge request through the Gitlab web interface # sync your fork with the upstream repository git fetch upstream # checkout the branch you want to sync (usually 'master' or 'release-X.Y.Z') git checkout <BRANCH> # merge all upstream changes which must be a fast-forward merge git merge --ff-only upstream/<BRANCH> # push the updates to your fork git push Checklist for merge requests Your branch has been rebased on the target branch and can be integrated through a fast-forward merge. A detailed description of the merge request is provided which includes a reference to a JIRA ticket (e.g. Closes ACTS-1234 , GitLab Help: Closing JIRA tickets ). All files start with the MPLv2 license statement. All newly introduced functions and classes have been documented properly with doxygen. Unit tests are provided for new functionalities. For bugfixes: a test case has been added to avoid the re-appearance of this bug in the future. All added cmake options were added to 'cmake/PrintOptions.cmake'. Workflow recommendations In the following a few recommendations are outlined which should help you to get familiar with development process in the Acts project. Each development its own branch! Branching in git is simple, it is fun and it helps you keep your working copy clean. Therefore, you should start a new branch for every development. All work which is logically/conceptually linked should happen in one branch. Keep your branches short. This helps immensly to understand the git history if you need to look at it in the future. If projects are complex (e.g. large code refactoring or complex new features), you may want to use sub -branches from the main development branch as illustrated in the picture below. 1. Never, ever directly work on any \"official\" branch! Though not strictly necessary and in the end it is up to you, it is strongly recommended that you never commit directly on a branch which tracks an \"official\" branch. As all branches are equal in git, the definition of \"official\" branch is quite subjective. In the Acts project you should not work directly on branches which are protected in the CERN GitLab repository. Usually, these are the master and release-X.Y.Z branches. The benefit of this strategy is that you will never have problems to update your fork. Any git merge in your local repository on such an \"official\" branch will always be a fast-forward merge. Use atomic commits! Similarly to the concept of branches, each commit should reflect a self-contained change. Try to avoid overly large commits (bad examples are for instance mixing logical change with code cleanup and typo fixes). Write good commit messages! Well-written commit messages are key to understand your changes. There are many guidelines available on how to write proper commit logs (e.g. here , here , or here ). As a short summary: Structure your commit messages into short title (max 50 characters) and longer description (max width 72 characters)! This is best achieved by avoiding the commit -m option. Instead write the commit message in an editor/git tool/IDE... Describe why you did the change (git diff already tells you what has changed)! Mention any side effects/implications/consquences! Prefer git pull --rebase! If you work with a colleague on a new development, you may want to include his latest changes. This is usually done by calling git pull which will synchronise your local working copy with the remote repository (which may have been updated by your colleague). By default, this action creates a merge commit if you have local commits which were not yet published to the remote repository. These merge commits are considered to contribute little information to the development process of the feature and they clutter the history (read more e.g. here or here ). This problem can be avoided by using git pull --rebase which replays your local (un-pushed) commits on the tip of the remote branch. You can make this the default behaviour by running git config pull.rebase true . More about merging vs rebasing can be found here . Push your development branches as late as possible! Unless required by other circumstances (e.g. collaboration with colleagues, code reviews etc) it is recommended to push your development branch once you are finished. This gives you more flexibility on what you can do with your local commits (e.g. rebase interactively) without affecting others. Thus, it minimises the risk for running into git rebase problems. Update the documentation! Make sure that the documentation is still valid after your changes. Perform updates where needed and ensure integrity between the code and its documentation. Coding style and guidelines The Acts project uses clang-format (currently v3.8.0) for formatting its source code. A .clang-format configuration file comes with the project and should be used to automatically format the code. There are several instructions available on how to integrate clang-format with your favourite IDE (e.g. eclipse , Xcode , emacs ). The Acts CI system will automatically apply code reformatting using the provided clang-format configuration once merge requests are opened. However, developers are strongly encouraged to use this code formatter also locally to reduce conflicts due to formatting issues. In addition, the following conventions are used in Acts code: Class names start with a capital letter. Function names start with a lower-case letter and use camel-case. Names of class member variables start with m_ . getter methods are called like the corresponding member variable without the prefix 'get' (e.g. covariance() instead of getCovariance() ) setter methods use the prefix 'set' (e.g. setCovariance(...) ) passing arguments to functions: by value for simple data types (e.g. int, float double, bool) by constant reference for required input of non-trivial type by (raw) pointer for optional input of non-trivial type only use smart pointers if the function called must handle ownership (very few functions actually do) returning results from functions: newly created objects should be returned a) as unique pointer if the object is of polymorphic type or its presence is not always ensured b) by value if the object is of non-polymorphic type and always exists existing objects (e.g. member variables) should be returned by a) const reference for custom types with costly copy constructors b) value in all other cases writing unit tests: place the source code of the test in the Tests directory, with following options: a) the naming of most unit test should be {ClassName} + Tests.cpp b) in case specific functionality of a single class is tested in different files, use {ClassName} + {TestType} + Tests.cpp c) case the written tests concerns CPU profiling, use {ClassName} + Benchmark.cpp add the test as {ClassName} + UnitTest or {ClassName} + {TestType} + UnitTest to the test suite for unit tests add the benchmark executable as {ClassName} + Benchmark to the bin/Profiling folder Doxygen documentation: Put all documentation in the header files. Use /// as block comment (instead of /* ... */ ). Doxygen documentation goes in front of the documented entity (class, function, (member) variable). Use the \\@<cmd> notation. Functions and classes must have the \\@brief description. Document all (template) parameters using \\@(t)param and explain the return value for non-void functions. Mention important conditions which may affect the return value. Use @remark to specify pre-conditions. Use @note to provide additional information. Link other related entities (e.g. functions) using @sa . git tips and tricks The following section gives some advise on how to solve certain situations you may encounter during your development process. Many of these commands have the potential to loose uncommitted data. So please make sure that you understand what you are doing before running the receipes below. Also, this collection is non-exhaustive and alternative approaches exist. If you want to contribute to this list, please drop an email to acts-developers@cern.ch . Before doing anything In the rare event that you end up in a situation you do not know how to solve, get to a clean state of working copy and create a (backup) branch, then switch back to the original branch. If anything goes wrong, you can always checkout the backup branch and you are back to where you started. Modify the author of a commit If your git client is not correctly set up on the machine you are working on, it may derive the committer name and email address from login and hostname information. In this case your commits are likely rejected by the CERN GitLab server. As a first step, you should correctly configure git on this machine as described above so that this problems does not appear again. a) You are lucky and only need to fix the author of the latest commit. You can use git commit --amend : git commit --amend --no-edit --author \"My Name <login@cern.ch>\" b) You need to fix (several) commit(s) which are not the current head. You can use git rebase : For the following it is assumed that all commits which need to be fixed are in the same branch <BRANCH>, and <SHA> is the hash of the earliest commit which needs to be corrected. git checkout <BRANCH> git rebase -i -p <SHA>^ In the editor opened by the git rebase procedure, add the following line after each commit you want to fix: exec git commit --amend --author=\"New Author Name <email@address.com>\" -C HEAD Then continue with the usual rebase procedure. Make a bugfix while working on a feature During the development of a new feature you discover a bug which needs to be fixed. In order to not mix bugfix and feature development, the bugfix should happen in a different branch. The recommended procedure for handling this situation is the following: 1. Get into a clean state of your working directory on your feature branche (either by commiting open changes or by stashing them). 1. Checkout the branch the bugfix should be merged into (either master or release-X.Y.Z ) and get the most recent version. 1. Create a new branch for the bugfix. 1. Fix the bug, write a test, update documentation etc. 1. Open a merge request for the bug fix. 1. Switch back to your feature branch. 1. Merge your local bugfix branch into the feature branch. Continue your feature development. 1. Eventually, the bugfix will be merged into master . Then, you can rebase your feature branch on master which will remove all duplicate commits related to the bugfix. In git commands this looks like: git stash git checkout master && git pull git checkout -b <bug_fix_branch> # Implement the bug fix, add tests, commit. # Open a merge request. git checkout <feature_branch> git merge <bug_fix_branch> # Once the merge request for the bug fix is accepted in the upstream repository: git fetch git rebase origin/master This should give the following git history where the initial feature branch is blue, the bugfix branch is yellow and the feature branch after the rebase is red. Move most recent commit(s) to new branch Very enthusiastic about the cool feature you are going to implement, you started from master and made one (or more) commits for your development. That's when you realised that you did not create a new branch and committed directly to the master branch. As you know that you should not directly commit to any \"official\" branch, you are desperately looking for a solution. Assuming your current situation is: A -> B -> C -> D -> E where HEAD is pointing to E (= master) and the last \"official\" commit is B as shown below. You can resolve this situation by running: git checkout <new_branch_name> git reset --hard <hash of B> git checkout <new_branch_name> which should give the following situation: Now, master is pointing to B, HEAD and <new_branch_name> are pointing to E and you can happily continue with your work. Review other contributions Approving a merge request Writing new code is not the only way one can contribute to the Acts project. Another greatly appreciated contribution is looking at other proposed contributions to the project. The more eyeballs look at a piece of code before it is merged into the Acts repository, the smaller the chances that a bug or other unwanted behaviour is accidentally introduced into the Acts codebase. This is why we require that every Acts merge request receives at least one human review before being merged. You can help reviewing proposed contributions by going to the \"merge requests\" section of the Acts Gitlab repository and having a look at the proposals that are being made here. The present contribution guide should serve as a good indication of what we expect from code submissions. In addition, please look at the merge request itself: Does its title and description reflect its contents? Do the automated continuous integration tests pass without problems? Have all the comments raised by previous reviewers been addressed? If you are confident that a merge request is ready for integration, please make it known by clicking the \"Approve merge request\" button of the Gitlab interface. This notifies other members of the Acts team of your decision, and marks the merge request as ready to be merged. Merging a merge request If you have been granted merge rights on the Acts repository, you can merge a merge request into the Acts master branch after it has been approved by someone else. Gitlab may warn you that a \"Fast-forward merge is not possible\". This warning means that the merge request has fallen behind the current Acts master branch, and should be updated through a rebase. Please notify the merge request author in order to make sure that the latest master changes do not affect the merge request, and to have it updated as appropriate. Administrator's corner This section gives useful information to the administrators of the Acts project. For normal developers the sections below are irrelevant.","title":"Contribution Guide"},{"location":"guides_contribution/#contributing-to-acts","text":"Contributions to the Acts project are very welcome and feedback on the documentation is greatly appreciated. In order to be able to contribute to the Acts project, developers must have a valid CERN user account. Unfortunately, lightweight CERN accounts for external users do not have sufficient permissions to access certain CERN services used by the Acts project. Mailing lists Bug reports and feature requests Make a contribution Preparation New development Creating a merge request Using forks Workflow recommendations Coding style and guidelines git tips and tricks Review other contributions Approving a merge request Merging a merge request Administrator's corner Making a new Acts release","title":"Contributing to Acts"},{"location":"guides_contribution/#mailing-lists","text":"acts-users@cern.ch : Users of the Acts project should subscribe to this list as it provides: regular updates on the software, access to the Acts JIRA project for bug fixes/feature requests, a common place for asking any kind of questions. acts-developers@cern.ch : Developers are encouraged to also subscribe to this list as it provides you with: a developer role in the Acts JIRA project (allows you to handle tickets), developer access to the Acts git repository information about developer meetings, a common place for technical discussions.","title":"Mailing lists"},{"location":"guides_contribution/#bug-reports-and-feature-requests","text":"If you want to report or start a feature request, please open a ticket in the Acts JIRA ( Note: access is restricted to members of the mailing lists mentioned above). A comprehensive explanation will help the development team to respond in a timely manner. Therefore, the following details should be mentioned: bug reports issue type: \"Bug\" summary: short description of the problem priority: will be set by the development team components: if known, part of Acts affected by this bug; leave empty otherwise affects version: version of Acts affected by this bug a detailed description of the bug including a receipe on how to reproduce it and any hints which may help diagnosing the problem feature requests issue type: \"Improvement\" or \"New Feature\" summary: short description of new feature priority: will be set by the development team a detailed description of the feature request including possible use cases and benefits for other users","title":"Bug reports and feature requests"},{"location":"guides_contribution/#make-a-contribution","text":"The instructions below should help you getting started with development process in the Acts project. If you have any questions, feel free to ask acts-developers@cern for help or guidance. The Acts project uses a git repository which is hosted on the CERN GitLab server. In order to be able to push your changes and to create merge requests (and thus, contribute to the development of Acts), you must be subscribed to acts-developers@cern.ch. A general introduction to the GitLab web interface can be found here . Very nice tutorials as well as explanations of concepts and workflows with git can be found on Atlassian . For a shorter introduction and the full git documentation have a look at the git tutorial .","title":"Make a contribution"},{"location":"guides_contribution/#preparation","text":"","title":"Preparation"},{"location":"guides_contribution/#configuring-git","text":"Commits to repositories on the CERN GitLab server are only accepted from CERN users. Therefore, it is important that git is correctly configured on all machines you are working on. It is necessary to that the git user email address to the primary email address of your CERN account (usually: firstname.lastname@cern.ch). You can check the current values with: git config user.name git config user.email You can change those settings by either editing the .gitconfig file in your home directory or by running: git config user.name git config --global user.name \"Donald Duck\" git config --global user.email \"donald.duck@cern.ch\" Further recommended settings are: git config user.name git config --global push.default simple git config --global pull.rebase true","title":"Configuring git"},{"location":"guides_contribution/#getting-a-local-working-copy","text":"As a first step, you need to clone the Acts repository which gives you a local, fully functional and self-contained git repository. This means you can work locally (commit changes, compare versions, inspect the history) while being offline. An internet connection is only required for pull ing in new updates or push ing your changes to the Gitlab server. git clone <ACTS_URL> <DESTINATION> <ACTS_URL> can be found on the project page of the Acts git repository . There are different URLs for different authentication methods. <DESTINATION> is optional and gives the path on your local machine where the clone will be created.","title":"Getting a local working copy"},{"location":"guides_contribution/#starting-a-new-development","text":"When you start a new development, you should make sure to start from the most recent version. Therefore, you need to fetch the latest changes from the remote repository by running from inside the Acts directory: git fetch origin Now you can create a new branch for your development using git checkout -b <BRANCH_NAME> <UPSTREAM> --no-track Let's go through the options one by one: -b <BRANCH_NAME> creates a new branch called <BRANCH_NAME>. <UPSTREAM> specifies the starting point of the new branch. For feature developments, this should be origin/master , for bug fixes in older releases it could also be for instance origin/release-0.03.X . --no-track decouples your local branch from the remote branch. This is helpful because you do not have the permissions to push any commits directly into the remote master / release-X,Y,Z branch.","title":"Starting a new development"},{"location":"guides_contribution/#creating-a-merge-request","text":"Once your development is ready for integration, you should open a merge request at the Acts project ( GitLab Help: Create a merge request ). The target branch should usually be master for feature requests and releas-X,Y,Z for bugfixes. The Acts projects accepts only fast-foward merges which means that your branch must have been rebased on the target branch. This can be achieved by fetching upstream changes and rebasing afterwards: git fetch origin git checkout <my_feature_branch> git rebase -i origin/<target_branch> git push -u origin <my_feature_branch> At this point you should make use of the interactive rebase procedure to clean up your commits (squash small fixes, improve commit messages etc; Rewriting history ). Merge requests are required to close a Acts JIRA ticket. This is achieved by adding e.g. 'fixes ACTS-XYZ' to the end of the merge request description. Please note that JIRA tickets should only be referenced by merge requests and not individual commits (since strictly there should only be one JIRA ticket per merge request). Once the merge request is opened, a continous integration job is triggered which will add multiple comments to the merge request (e.g. build status, missing license statements, doxygen errors, test coverage etc). Please have a look at them and fix them by adding more commits to your branch if needed. Please find below a short checklist for merge requests.","title":"Creating a merge request"},{"location":"guides_contribution/#using-forks","text":"Users or groups with git experience may prefer to use their own forks of the Acts project to develop and test their changes before they are integrated back into the main Acts repository. We assume that you are experienced with a fork-based workflow in git and only summarise the most important commands at the end of this section. As a first step, you need to create your own fork of the Acts project. For doing this, please go to the Acts GitLab page , click on the fork button, and follow the instructions ( GitLab Help \"How to fork a project\" ). To enable the CI on your fork, go from your project page to \"Settings -> CI / CD\", expand the \"Runners\" menu and click on \"Enable shared Runners\" for this project. # adding the central Acts repository as remote git remote add upstream ssh://git@gitlab.cern.ch:7999/acts/acts-core.git # get latest upstream changes git fetch upstream # start a new development git checkout -b <BRANCH_NAME> upstream/master --no-track # code, test, document # publish your changes to your fork git push -u origin <BRANCH_NAME> # open a merge request through the Gitlab web interface # sync your fork with the upstream repository git fetch upstream # checkout the branch you want to sync (usually 'master' or 'release-X.Y.Z') git checkout <BRANCH> # merge all upstream changes which must be a fast-forward merge git merge --ff-only upstream/<BRANCH> # push the updates to your fork git push","title":"Using forks"},{"location":"guides_contribution/#checklist-for-merge-requests","text":"Your branch has been rebased on the target branch and can be integrated through a fast-forward merge. A detailed description of the merge request is provided which includes a reference to a JIRA ticket (e.g. Closes ACTS-1234 , GitLab Help: Closing JIRA tickets ). All files start with the MPLv2 license statement. All newly introduced functions and classes have been documented properly with doxygen. Unit tests are provided for new functionalities. For bugfixes: a test case has been added to avoid the re-appearance of this bug in the future. All added cmake options were added to 'cmake/PrintOptions.cmake'.","title":"Checklist for merge requests"},{"location":"guides_contribution/#workflow-recommendations","text":"In the following a few recommendations are outlined which should help you to get familiar with development process in the Acts project. Each development its own branch! Branching in git is simple, it is fun and it helps you keep your working copy clean. Therefore, you should start a new branch for every development. All work which is logically/conceptually linked should happen in one branch. Keep your branches short. This helps immensly to understand the git history if you need to look at it in the future. If projects are complex (e.g. large code refactoring or complex new features), you may want to use sub -branches from the main development branch as illustrated in the picture below. 1. Never, ever directly work on any \"official\" branch! Though not strictly necessary and in the end it is up to you, it is strongly recommended that you never commit directly on a branch which tracks an \"official\" branch. As all branches are equal in git, the definition of \"official\" branch is quite subjective. In the Acts project you should not work directly on branches which are protected in the CERN GitLab repository. Usually, these are the master and release-X.Y.Z branches. The benefit of this strategy is that you will never have problems to update your fork. Any git merge in your local repository on such an \"official\" branch will always be a fast-forward merge. Use atomic commits! Similarly to the concept of branches, each commit should reflect a self-contained change. Try to avoid overly large commits (bad examples are for instance mixing logical change with code cleanup and typo fixes). Write good commit messages! Well-written commit messages are key to understand your changes. There are many guidelines available on how to write proper commit logs (e.g. here , here , or here ). As a short summary: Structure your commit messages into short title (max 50 characters) and longer description (max width 72 characters)! This is best achieved by avoiding the commit -m option. Instead write the commit message in an editor/git tool/IDE... Describe why you did the change (git diff already tells you what has changed)! Mention any side effects/implications/consquences! Prefer git pull --rebase! If you work with a colleague on a new development, you may want to include his latest changes. This is usually done by calling git pull which will synchronise your local working copy with the remote repository (which may have been updated by your colleague). By default, this action creates a merge commit if you have local commits which were not yet published to the remote repository. These merge commits are considered to contribute little information to the development process of the feature and they clutter the history (read more e.g. here or here ). This problem can be avoided by using git pull --rebase which replays your local (un-pushed) commits on the tip of the remote branch. You can make this the default behaviour by running git config pull.rebase true . More about merging vs rebasing can be found here . Push your development branches as late as possible! Unless required by other circumstances (e.g. collaboration with colleagues, code reviews etc) it is recommended to push your development branch once you are finished. This gives you more flexibility on what you can do with your local commits (e.g. rebase interactively) without affecting others. Thus, it minimises the risk for running into git rebase problems. Update the documentation! Make sure that the documentation is still valid after your changes. Perform updates where needed and ensure integrity between the code and its documentation.","title":"Workflow recommendations"},{"location":"guides_contribution/#coding-style-and-guidelines","text":"The Acts project uses clang-format (currently v3.8.0) for formatting its source code. A .clang-format configuration file comes with the project and should be used to automatically format the code. There are several instructions available on how to integrate clang-format with your favourite IDE (e.g. eclipse , Xcode , emacs ). The Acts CI system will automatically apply code reformatting using the provided clang-format configuration once merge requests are opened. However, developers are strongly encouraged to use this code formatter also locally to reduce conflicts due to formatting issues. In addition, the following conventions are used in Acts code: Class names start with a capital letter. Function names start with a lower-case letter and use camel-case. Names of class member variables start with m_ . getter methods are called like the corresponding member variable without the prefix 'get' (e.g. covariance() instead of getCovariance() ) setter methods use the prefix 'set' (e.g. setCovariance(...) ) passing arguments to functions: by value for simple data types (e.g. int, float double, bool) by constant reference for required input of non-trivial type by (raw) pointer for optional input of non-trivial type only use smart pointers if the function called must handle ownership (very few functions actually do) returning results from functions: newly created objects should be returned a) as unique pointer if the object is of polymorphic type or its presence is not always ensured b) by value if the object is of non-polymorphic type and always exists existing objects (e.g. member variables) should be returned by a) const reference for custom types with costly copy constructors b) value in all other cases writing unit tests: place the source code of the test in the Tests directory, with following options: a) the naming of most unit test should be {ClassName} + Tests.cpp b) in case specific functionality of a single class is tested in different files, use {ClassName} + {TestType} + Tests.cpp c) case the written tests concerns CPU profiling, use {ClassName} + Benchmark.cpp add the test as {ClassName} + UnitTest or {ClassName} + {TestType} + UnitTest to the test suite for unit tests add the benchmark executable as {ClassName} + Benchmark to the bin/Profiling folder Doxygen documentation: Put all documentation in the header files. Use /// as block comment (instead of /* ... */ ). Doxygen documentation goes in front of the documented entity (class, function, (member) variable). Use the \\@<cmd> notation. Functions and classes must have the \\@brief description. Document all (template) parameters using \\@(t)param and explain the return value for non-void functions. Mention important conditions which may affect the return value. Use @remark to specify pre-conditions. Use @note to provide additional information. Link other related entities (e.g. functions) using @sa .","title":"Coding style and guidelines"},{"location":"guides_contribution/#git-tips-and-tricks","text":"The following section gives some advise on how to solve certain situations you may encounter during your development process. Many of these commands have the potential to loose uncommitted data. So please make sure that you understand what you are doing before running the receipes below. Also, this collection is non-exhaustive and alternative approaches exist. If you want to contribute to this list, please drop an email to acts-developers@cern.ch . Before doing anything In the rare event that you end up in a situation you do not know how to solve, get to a clean state of working copy and create a (backup) branch, then switch back to the original branch. If anything goes wrong, you can always checkout the backup branch and you are back to where you started. Modify the author of a commit If your git client is not correctly set up on the machine you are working on, it may derive the committer name and email address from login and hostname information. In this case your commits are likely rejected by the CERN GitLab server. As a first step, you should correctly configure git on this machine as described above so that this problems does not appear again. a) You are lucky and only need to fix the author of the latest commit. You can use git commit --amend : git commit --amend --no-edit --author \"My Name <login@cern.ch>\" b) You need to fix (several) commit(s) which are not the current head. You can use git rebase : For the following it is assumed that all commits which need to be fixed are in the same branch <BRANCH>, and <SHA> is the hash of the earliest commit which needs to be corrected. git checkout <BRANCH> git rebase -i -p <SHA>^ In the editor opened by the git rebase procedure, add the following line after each commit you want to fix: exec git commit --amend --author=\"New Author Name <email@address.com>\" -C HEAD Then continue with the usual rebase procedure. Make a bugfix while working on a feature During the development of a new feature you discover a bug which needs to be fixed. In order to not mix bugfix and feature development, the bugfix should happen in a different branch. The recommended procedure for handling this situation is the following: 1. Get into a clean state of your working directory on your feature branche (either by commiting open changes or by stashing them). 1. Checkout the branch the bugfix should be merged into (either master or release-X.Y.Z ) and get the most recent version. 1. Create a new branch for the bugfix. 1. Fix the bug, write a test, update documentation etc. 1. Open a merge request for the bug fix. 1. Switch back to your feature branch. 1. Merge your local bugfix branch into the feature branch. Continue your feature development. 1. Eventually, the bugfix will be merged into master . Then, you can rebase your feature branch on master which will remove all duplicate commits related to the bugfix. In git commands this looks like: git stash git checkout master && git pull git checkout -b <bug_fix_branch> # Implement the bug fix, add tests, commit. # Open a merge request. git checkout <feature_branch> git merge <bug_fix_branch> # Once the merge request for the bug fix is accepted in the upstream repository: git fetch git rebase origin/master This should give the following git history where the initial feature branch is blue, the bugfix branch is yellow and the feature branch after the rebase is red. Move most recent commit(s) to new branch Very enthusiastic about the cool feature you are going to implement, you started from master and made one (or more) commits for your development. That's when you realised that you did not create a new branch and committed directly to the master branch. As you know that you should not directly commit to any \"official\" branch, you are desperately looking for a solution. Assuming your current situation is: A -> B -> C -> D -> E where HEAD is pointing to E (= master) and the last \"official\" commit is B as shown below. You can resolve this situation by running: git checkout <new_branch_name> git reset --hard <hash of B> git checkout <new_branch_name> which should give the following situation: Now, master is pointing to B, HEAD and <new_branch_name> are pointing to E and you can happily continue with your work.","title":"git tips and tricks"},{"location":"guides_contribution/#review-other-contributions","text":"","title":"Review other contributions"},{"location":"guides_contribution/#approving-a-merge-request","text":"Writing new code is not the only way one can contribute to the Acts project. Another greatly appreciated contribution is looking at other proposed contributions to the project. The more eyeballs look at a piece of code before it is merged into the Acts repository, the smaller the chances that a bug or other unwanted behaviour is accidentally introduced into the Acts codebase. This is why we require that every Acts merge request receives at least one human review before being merged. You can help reviewing proposed contributions by going to the \"merge requests\" section of the Acts Gitlab repository and having a look at the proposals that are being made here. The present contribution guide should serve as a good indication of what we expect from code submissions. In addition, please look at the merge request itself: Does its title and description reflect its contents? Do the automated continuous integration tests pass without problems? Have all the comments raised by previous reviewers been addressed? If you are confident that a merge request is ready for integration, please make it known by clicking the \"Approve merge request\" button of the Gitlab interface. This notifies other members of the Acts team of your decision, and marks the merge request as ready to be merged.","title":"Approving a merge request"},{"location":"guides_contribution/#merging-a-merge-request","text":"If you have been granted merge rights on the Acts repository, you can merge a merge request into the Acts master branch after it has been approved by someone else. Gitlab may warn you that a \"Fast-forward merge is not possible\". This warning means that the merge request has fallen behind the current Acts master branch, and should be updated through a rebase. Please notify the merge request author in order to make sure that the latest master changes do not affect the merge request, and to have it updated as appropriate.","title":"Merging a merge request"},{"location":"guides_contribution/#administrators-corner","text":"This section gives useful information to the administrators of the Acts project. For normal developers the sections below are irrelevant.","title":"Administrator's corner"},{"location":"guides_developers/","text":"Developer Guidelines for Acts Writing thread-safe code Acts aims to be readily usable from multi-threaded event reconstruction frameworks. To achieve this goal, all internal Acts state should either be kept private to a thread or remain constant after initialization. This is supported by aiming at a strict discipline of const-correctness (i.e. data reached via a const pointer cannot be mutated), the only allowed exceptions being deferred initialization code which does not run in parallel such as the geometry building process. Const correctness Very simple rules for const correctness should be followed by convention. As every tool that performs an operation is permitted to have a internal state, every method has to be declared const as it is not allowed to alter the sate of the exectuing tool. /// Method that performs an operation /// /// @param input variable that is needed by this method /// /// @return a direct output of this method return_type doSomethingWithoutCache(const input_type& input) const; Stateless tools and visitor cache It is indeed sometimes useful and needed to cache certain values or parameters for repetitive use. For example, track propagation requires access to the detector's magnetic field, which can be sped up by caching intermediary computations across extrapolation steps. Any tool in Acts that requires a cache must follow a visitor pattern design, i.e. the caller provides the cache in the function call and thus guarantees that the cache is thread-local. The Cache struct has to be done as a nested struct of the class and is being called as such. As a convention, the cache object is usually the first one provided in the method signature as a Cache& /// An Acts Tool that does some well defined job class MyTool { public: /// Cache for Acts tool /// /// it is exposed to public for use of the expert-only /// propagate_with_cache method of the propagator struct Cache { variable_type var; ///< the local cache } /// Method that performs an operation and also relies on a cache /// /// @param cache object /// @param input variable that is needed by this method /// /// @return a direct output of this method return_type doSomethingWithCache(Cache& cachem, const input_type& input) const; }; Configuration of Acts tools Configuration in Acts is done via a dedicated nested configuration struct which then is used for constructing the tool itself. By convention, this struct is called Config . /// An Acts Tool that does some well defined job class MyTool { public: /// Configuration struct struct Config { variable_type parameter; }; /// Tool constructor /// /// @param cfg is the configuration struct MyTool(const Config&cfg) : m_cfg(cfg){} private: Config m_cfg; ///< the configuration object }; This Config object can then be used to create and configure the tool. In case Acts is embedded in an experiment framework, the Config public members have to be connected to the framework configuration. // Create a config object and set my configuration parameter MyTool::Config mtConfig; mtConig.parameter = 3.1415927; // Now create a tool with this configuration MyTool mt(mtConfig);","title":"Developers Guide"},{"location":"guides_developers/#developer-guidelines-for-acts","text":"","title":"Developer Guidelines for Acts"},{"location":"guides_developers/#writing-thread-safe-code","text":"Acts aims to be readily usable from multi-threaded event reconstruction frameworks. To achieve this goal, all internal Acts state should either be kept private to a thread or remain constant after initialization. This is supported by aiming at a strict discipline of const-correctness (i.e. data reached via a const pointer cannot be mutated), the only allowed exceptions being deferred initialization code which does not run in parallel such as the geometry building process.","title":"Writing thread-safe code"},{"location":"guides_developers/#const-correctness","text":"Very simple rules for const correctness should be followed by convention. As every tool that performs an operation is permitted to have a internal state, every method has to be declared const as it is not allowed to alter the sate of the exectuing tool. /// Method that performs an operation /// /// @param input variable that is needed by this method /// /// @return a direct output of this method return_type doSomethingWithoutCache(const input_type& input) const;","title":"Const correctness"},{"location":"guides_developers/#stateless-tools-and-visitor-cache","text":"It is indeed sometimes useful and needed to cache certain values or parameters for repetitive use. For example, track propagation requires access to the detector's magnetic field, which can be sped up by caching intermediary computations across extrapolation steps. Any tool in Acts that requires a cache must follow a visitor pattern design, i.e. the caller provides the cache in the function call and thus guarantees that the cache is thread-local. The Cache struct has to be done as a nested struct of the class and is being called as such. As a convention, the cache object is usually the first one provided in the method signature as a Cache& /// An Acts Tool that does some well defined job class MyTool { public: /// Cache for Acts tool /// /// it is exposed to public for use of the expert-only /// propagate_with_cache method of the propagator struct Cache { variable_type var; ///< the local cache } /// Method that performs an operation and also relies on a cache /// /// @param cache object /// @param input variable that is needed by this method /// /// @return a direct output of this method return_type doSomethingWithCache(Cache& cachem, const input_type& input) const; };","title":"Stateless tools and visitor cache"},{"location":"guides_developers/#configuration-of-acts-tools","text":"Configuration in Acts is done via a dedicated nested configuration struct which then is used for constructing the tool itself. By convention, this struct is called Config . /// An Acts Tool that does some well defined job class MyTool { public: /// Configuration struct struct Config { variable_type parameter; }; /// Tool constructor /// /// @param cfg is the configuration struct MyTool(const Config&cfg) : m_cfg(cfg){} private: Config m_cfg; ///< the configuration object }; This Config object can then be used to create and configure the tool. In case Acts is embedded in an experiment framework, the Config public members have to be connected to the framework configuration. // Create a config object and set my configuration parameter MyTool::Config mtConfig; mtConig.parameter = 3.1415927; // Now create a tool with this configuration MyTool mt(mtConfig);","title":"Configuration of Acts tools"},{"location":"guides_framework/","text":"Framework and Examples The framework purpose Framework design Steering Examples Hello World Geometry Building Propagation Example Material Mapping and Material Validation Material Mapping Material Validation Framework and Examples Disclaimer This needs currently the origin/Remove_Legacy_New_Material branch from acts-framework and the according sub modules. The framework purpose Disclaimer The acts-framework is not designed for any production purpose, it is simply a helper framework that allows to test code from acts-core and acts-fatras in a parallel environment. Framework design The stucture of the acts-framework is inspired by Gaudi, but in general very rudimentary. It provides a parallel Sequencer , a central store (called WhiteBoard ) an algorithm sequence and a reader and writer structure. The Sequencer calls execute on the list of algorithms in the algorithm list. To allow parallel events to be processed, a dedicated EventContext object carries the event number and enforced reproducability concerning random number generation. Readers and writers need to be protected with a mutex in such a parallel structure. The multithreading is based on the tbb library, which is a requirement for the acts-framework . The number of threads is steered by the environment variable ACTSFW_NUM_THREADS . Steering The steering of examples is done using the boost_program_options which are defined for each example, depending on the given functionality. By calling a program with the --help the list of available options is usually printed. Examples Hello World The most rudimentary example to run is traditionally the HelloWorld example. It demonstrates how to create a simple algorithm, the HelloWorld algorithm which is attached as the single algorithm to the senquencer. For this reason it is explained here in a bit more detail. FW::ProcessCode FWE::HelloWorldAlgorithm::execute(FW::AlgorithmContext context) const { ACTS_INFO(\" Hello World! (from event \" << context.eventNumber << \")\"); ACTS_DEBUG(\" - that's an ACTS_DEBUG message\"); ACTS_VERBOSE(\" - that's an ACTS_VERBOSE message\"); return FW::ProcessCode::SUCCESS; } Here is the full program: it starts with the adding of common program options, such as the number of events and the output log level. Then, these parameters are read back in from the argument list provided. A HelloWorld algorithm is created and added to the sequencer, and finally the chosen nEvents are executed. /// Main read evgen executable /// /// @param argc The argument count /// @param argv The argument list int main(int argc, char* argv[]) { // Declare the supported program options. po::options_description desc(\"Allowed options\"); // Add the standard options FW::Options::addCommonOptions<po::options_description>(desc); // Map to store the given program options po::variables_map vm; // Get all options from contain line and store it into the map po::store(po::parse_command_line(argc, argv, desc), vm); po::notify(vm); // Print help if reqested if (vm.count(\"help\")) { std::cout << desc << std::endl; return 1; } // Read the common options auto nEvents = FW::Options::readNumberOfEvents<po::variables_map>(vm); auto logLevel = FW::Options::readLogLevel<po::variables_map>(vm); // And add the hello world algorithm std::shared_ptr<FW::IAlgorithm> hWorld( new FWE::HelloWorldAlgorithm(logLevel)); // Create the config object for the sequencer FW::Sequencer::Config seqConfig; // Now create the sequencer FW::Sequencer sequencer(seqConfig); sequencer.appendEventAlgorithms({hWorld}); sequencer.run(nEvents); // Return 0 for success return 0; } Let us first execute the binary with the --help option which prints out the available options for this executable. Allowed options: --help Produce help message -n [ --events ] arg (=1) The number of events to be processed -l [ --loglevel ] arg (=2) The output log level. Please set the wished number (0 = VERBOSE, 1 = DEBUG, 2 = INFO, 3 = WARNING, 4 = ERROR, 5 = FATAL). Only the number of events and the output log level can be changed for the HelloWorld example, which when executed with default parameters yields acts-tester$ ./ACTFWHelloWorldExample 14:38:38 Sequencer INFO Appended algorithm HelloWorld 14:38:38 Sequencer INFO Starting event loop for 14:38:38 Sequencer INFO 0 services 14:38:38 Sequencer INFO 0 readers 14:38:38 Sequencer INFO 0 writers 14:38:38 Sequencer INFO 1 algorithms 14:38:38 Sequencer INFO Run the event loop 14:38:38 Sequencer INFO start event 0 14:38:38 HelloWorld INFO Hello World! (from event 0) 14:38:38 Sequencer INFO event 0 done 14:38:38 Sequencer INFO Running end-of-run hooks of writers and services One can now modify the number of events and the screen output level, e.g. acts-tester$ ./ACTFWHelloWorldExample -l0 14:39:37 Sequencer INFO Appended algorithm HelloWorld 14:39:37 Sequencer INFO Starting event loop for 14:39:37 Sequencer INFO 0 services 14:39:37 Sequencer INFO 0 readers 14:39:37 Sequencer INFO 0 writers 14:39:37 Sequencer INFO 1 algorithms 14:39:37 Sequencer INFO Run the event loop 14:39:37 Sequencer INFO start event 0 14:39:37 HelloWorld INFO Hello World! (from event 0) 14:39:37 HelloWorld DEBUG - that is an ACTS_DEBUG message 14:39:37 HelloWorld VERBOSE - that is an ACTS_VERBOSE message 14:39:37 Sequencer INFO event 0 done 14:39:37 Sequencer INFO Running end-of-run hooks of writers and services or (with export ACTSFW_NUM_THREADS=1 ): acts-tester$ ./ACTFWHelloWorldExample -n5 14:41:15 Sequencer INFO Appended algorithm HelloWorld 14:41:15 Sequencer INFO Starting event loop for 14:41:15 Sequencer INFO 0 services 14:41:15 Sequencer INFO 0 readers 14:41:15 Sequencer INFO 0 writers 14:41:15 Sequencer INFO 1 algorithms 14:41:15 Sequencer INFO Run the event loop 14:41:15 Sequencer INFO start event 0 14:41:15 HelloWorld INFO Hello World! (from event 0) 14:41:15 Sequencer INFO event 0 done 14:41:15 Sequencer INFO start event 1 14:41:15 HelloWorld INFO Hello World! (from event 1) 14:41:15 Sequencer INFO event 1 done 14:41:15 Sequencer INFO start event 2 14:41:15 HelloWorld INFO Hello World! (from event 2) 14:41:15 Sequencer INFO event 2 done 14:41:15 Sequencer INFO start event 3 14:41:15 HelloWorld INFO Hello World! (from event 3) 14:41:15 Sequencer INFO event 3 done 14:41:15 Sequencer INFO start event 4 14:41:15 HelloWorld INFO Hello World! (from event 4) 14:41:15 Sequencer INFO event 4 done 14:41:15 Sequencer INFO Running end-of-run hooks of writers and service When increasing the available number of threads, one can see how the events get executed in parallel and the screen print out gets scrambled: acts-tester$ ./ACTFWHelloWorldExample -n5 14:43:23 Sequencer INFO Appended algorithm HelloWorld 14:43:23 Sequencer INFO Starting event loop for 14:43:23 Sequencer INFO 0 services 14:43:23 Sequencer INFO 0 readers 14:43:23 Sequencer INFO 0 writers 14:43:23 Sequencer INFO 1 algorithms 14:43:23 Sequencer INFO Run the event loop 14:43:23 Sequencer INFO start event 2 14:43:23 Sequencer INFO start event 3 14:43:23 Sequencer INFO start event 0 14:43:23 HelloWorld INFO Hello World! (from event 2) 14:43:23 HelloWorld INFO Hello World! (from event 3) 14:43:23 Sequencer INFO start event 4 14:43:23 Sequencer INFO event 3 done 14:43:23 HelloWorld INFO Hello World! (from event 4) 14:43:23 Sequencer INFO start event 1 14:43:23 HelloWorld INFO Hello World! (from event 0) 14:43:23 Sequencer INFO event 2 done 14:43:23 Sequencer INFO event 4 done 14:43:23 Sequencer INFO event 0 done 14:43:23 HelloWorld INFO Hello World! (from event 1) 14:43:23 Sequencer INFO event 1 done 14:43:23 Sequencer INFO Running end-of-run hooks of writers and services Geometry Building This example does not run an event loop, it only build the geometry and evokes writers for the output of the geometry into some visualisation format ( .obj ). There are different ways to build an Acts geometry, depending on which input geometry is chosen: GenericGeometry, i.e. an adhoc geometry descrbed in Detectors/GenericGeometry using an explicit C++ description RootGeometry, a TGeo/ROOT based geometry constructed with the TGeoLayerBuilder from acts-core DD4hepGeometry, a TGeo/DD4hep based geometry constructed with the DD4hepLayerBuilder from acts-core Let us start with the ACTFWGenericGeometryExample , and inspect the options: ./ACTFWGenericGeometryExample --help Allowed options: --help Produce help message -n [ --events ] arg (=1) The number of events to be processed -l [ --loglevel ] arg (=2) The output log level. Please set the wished number (0 = VERBOSE, 1 = DEBUG, 2 = INFO, 3 = WARNING, 4 = ERROR, 5 = FATAL). --obj-tg-fileheader arg The (optional) file header for the tracking geometry. --obj-tg-sensitiveheader arg The (optional) header in front of sensitive sensors. --obj-tg-layerheader arg The (optional) header in front of layer surfaces. --obj-sf-fileheader arg The (optional) file header for the surface writer. --obj-sf-phisegments arg (=72) Number of phi segments to approximate curves. --obj-sf-outputPrecission arg (=6) Floating number output precission. --obj-sf-outputScalor arg (=1) Scale factor to be applied. --obj-sf-outputThickness arg (=1) The surface thickness. --obj-sf-outputSensitive arg (=1) Write sensitive surfaces. --obj-sf-outputLayers arg (=1) Write layer surfaces. --geo-surface-loglevel arg (=3) The outoput log level for the surface building. --geo-layer-loglevel arg (=3) The output log level for the layer building. --geo-volume-loglevel arg (=3) The output log level for the volume building. --geo-subdetectors arg (= ) Sub detectors for the output writing --geo-material-mode arg (=1) Modes are: 0 (none), 1 (construct), 2 (load), 3 (proto) --geo-material-file arg (=material-maps.root) File for the material maps to be loaded. --output-dir arg Output directory location. --output-root arg (=0) Switch on to write '.root' output file(s). --output-csv arg (=0) Switch on to write '.csv' output file(s). --output-obj arg (=0) Switch on to write '.obj' ouput file(s). --output-json arg (=0) Switch on to write '.json' ouput file(s). There are different steering options concering the output, most importantly the --geo-subdetectors option that reads the list of sub detectors which should be written out for this detector. The standard GenericGeometry is built from a beam pipe, a pixel detector, a pixel support tube, short strips detector (SStrip) and long strings (LStrip), hence we can evoke the following call. ./ACTFWGenericGeometryExample --output-obj 1 --geo-subdetectors BeamPipe Pixel PST SStrip LStrip This creates dedicated obj files that can be used for displaying the detector with a standard viewer that can read the obj format: -rw-r--r-- 1 salzburg staff 13542 Jan 15 16:59 BeamPipe.obj -rw-r--r-- 1 salzburg staff 3095801 Jan 15 16:59 LStrip.obj -rw-r--r-- 1 salzburg staff 13580 Jan 15 16:59 PST.obj -rw-r--r-- 1 salzburg staff 1963033 Jan 15 16:59 Pixel.obj -rw-r--r-- 1 salzburg staff 3543361 Jan 15 16:59 SStrip.obj The following is an example of Pixel.obj displayed with a standard online obj viewer With appropriate input, the ACTFWRootGeometryExample and DD4hepGeometryExample are to be used in a very similar way. Propagation Example The PropagationExample again exists for the different geoemtry backends and is based on the Algorithms/PropagationAlgorithm module. It performs random test propagations throught he detector and allows to write them with certain debug information into appropriate writers. Let us continue with the Generic geometry example, and first inspect he options: next to the geometry building options ( --geo-xxx-yyy ) there appear options for the magnetic field setup ( --bf-xxx-yyy ) and for the behaviour of the propagation ( --prop-xx-yy ). Running actstest$ ./ACTFWGenericPropagationExample -n 10 --prop-ntests 10000 --bf-values 0 0 2 --output-root 1 Performs 10 events of each 10000 test propagations through a magnetic field of 2 Tesla in z direction and produces a root output file (by default propagation-steps.root ) with the default EigenStepper (Runge-Kutta-Nystrom integration). Inspecting the root files reveals the detector and its interaction with the runge kutta propagation algorithm, the following pictures shows all steps the stepper has performed, either due to navigation instruction, accuracy control, abort condition and user setting: Restricting the steps to only those performed for navigational purposes, shows: And, furthermore, requiring that sensitive material was hit, reveils: Which leaves the steps that are necessary due to accuracy control to be: One can clearly see how lower momentum tracks need more steps. Material Mapping and Material Validation The Acts toolkit consists of an automated system to map a complex detector geometry onto a more suitable, simplified geometry model for reconstruction. Material Mapping The MaterialMapping example demonstrates how to map and project material information from a complex detector geometry onto a set of chosen layers. As an input to the MaterialMapping algorithm a collection of recorded material information is needed. The input format for the mapper is a collection of MaterialInteraction objects, this is usually generated with Geant4 and an appropriate G4UserAction can be found in the acts-framework/Plugins/Geant4 plugin. Alternatively, the MaterialInteractor of the acts-core module can also write out such a collection. For the purpose of this example, we use the ad-hoc built material description of the GenericDetector , which has certain material properties on sensitive elements and support structure, such as on the beam pipe and the PST. To create an input collection, one just needs to run the ACTFWGenericMaterialValidationExample and write out the recorded material with the root output option. ./ACTFWGenericMaterialValidationExample -n 1000 --prop-ntests 1000 --prop-energyloss 0 --prop-scattering 0 --prop-record-material 1 --output-root 1 This executes 1000 events with each 1000 straight line tracks using the PropagationAlgorithm with energy loss and scattering switched off, but configured to record material and output a root file (default: propagation-material.root ). Let's inspect the root file, by plotting all position in a central region where material was found One can clearly see the support structures and the sensitive surface carrying material. We can also look at the total amount of material in this tracker, here shown in thickness in terms of radiation length: Now, let us use this input file to map it onto a simpler configuration. The generic detector is designed to do so. When chosing the option --geo-material-mode=3 , the exact same detector will be built, but instead of a material descriptionb for sensitive surfaces and support structures, only certain layers are loaded with a so called ProtoSurfaceMaterial object, which is a placeholder for the mapping. It consists of a binning prescription, i.e. a directive how granular one wants the maps on this layer to be done. Let us now run the mapping process with the more complicated input from the GenericDetector and map it onto a few surfaces with a dedicated binning structure. ./ACTFWGenericMaterialMappingExample -n 1000000 --geo-material-mode=3 --output-root 1 --input-root 1 --input-files propagation-material.root From our prior validation job of 1000 events times 1000 tests we have 1000000 mapping events available. ATTENTION: Since the mapping process combines information from all events into a single collection of material maps, this has to be run in single trheaded mode, i.e. with export ACTSFW_NUM_THREADS=1 . At the end of this job, some screen information shows which maps have been produced: 15:18:11 Sequencer INFO Running end-of-run hooks of writers and services 15:18:11 IndexedMater INFO Writing out map at Material_vol5_lay2_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay2_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay4_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay6_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay8_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay10_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay12_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay14_app0_sen0 And, in this configuration, a root file containing the maps is produced: -rw-r--r-- 1 salzburg staff 1430046 Jan 16 15:18 material-maps.root We can have a look at those maps and even plot them: One can see, how the module structure with its overlaps in now projected into one binned material map. Material Validation The MaterialValidation example is built upon the PropagationAlgorithm : it runs test propagations and records the material inforamtion in the tracker. If configured with the --root-output 1 option, this iformation is written into a root file. It has already be used in the MaterialMapping example to produce the material information that is then later mapped onto the ProtoSurfaceMaterial to produce the material maps to be loaded. After having produces such material maps, we can now create the geometry with loaded maps and compare the material budget to the orginal material source. ./ACTFWGenericMaterialValidationExample -n 100 --prop-ntests 1000 --prop-energyloss 0 --prop-scattering 0 --prop-record-material 1 --output-root 1 --geo-material-mode 2 --geo-material-file=material-maps.root The options --geo-material-mode 2 and --geo-material-file=material-maps.root steer hereby to load the material back from a file and do not construct it ad-hoc. Let us compare the material recorded with the original detector (black) and the maps (red), first the positions: One can perfectly see how the original map was more detailed and complicated and how the maps concentrate the material on simplified surfaces. However, when comparing the total material budget in the detector, only little compromise has been done and due to the binning structure of the maps, the structure in both pseudorapidity and azimuthal angle is pretty much preserved: This result can improved by optimising the binning structure.","title":"Framework Examples Guide"},{"location":"guides_framework/#framework-and-examples","text":"Disclaimer This needs currently the origin/Remove_Legacy_New_Material branch from acts-framework and the according sub modules.","title":"Framework and Examples"},{"location":"guides_framework/#the-framework-purpose","text":"Disclaimer The acts-framework is not designed for any production purpose, it is simply a helper framework that allows to test code from acts-core and acts-fatras in a parallel environment.","title":"The framework purpose"},{"location":"guides_framework/#framework-design","text":"The stucture of the acts-framework is inspired by Gaudi, but in general very rudimentary. It provides a parallel Sequencer , a central store (called WhiteBoard ) an algorithm sequence and a reader and writer structure. The Sequencer calls execute on the list of algorithms in the algorithm list. To allow parallel events to be processed, a dedicated EventContext object carries the event number and enforced reproducability concerning random number generation. Readers and writers need to be protected with a mutex in such a parallel structure. The multithreading is based on the tbb library, which is a requirement for the acts-framework . The number of threads is steered by the environment variable ACTSFW_NUM_THREADS .","title":"Framework design"},{"location":"guides_framework/#steering","text":"The steering of examples is done using the boost_program_options which are defined for each example, depending on the given functionality. By calling a program with the --help the list of available options is usually printed.","title":"Steering"},{"location":"guides_framework/#examples","text":"","title":"Examples"},{"location":"guides_framework/#hello-world","text":"The most rudimentary example to run is traditionally the HelloWorld example. It demonstrates how to create a simple algorithm, the HelloWorld algorithm which is attached as the single algorithm to the senquencer. For this reason it is explained here in a bit more detail. FW::ProcessCode FWE::HelloWorldAlgorithm::execute(FW::AlgorithmContext context) const { ACTS_INFO(\" Hello World! (from event \" << context.eventNumber << \")\"); ACTS_DEBUG(\" - that's an ACTS_DEBUG message\"); ACTS_VERBOSE(\" - that's an ACTS_VERBOSE message\"); return FW::ProcessCode::SUCCESS; } Here is the full program: it starts with the adding of common program options, such as the number of events and the output log level. Then, these parameters are read back in from the argument list provided. A HelloWorld algorithm is created and added to the sequencer, and finally the chosen nEvents are executed. /// Main read evgen executable /// /// @param argc The argument count /// @param argv The argument list int main(int argc, char* argv[]) { // Declare the supported program options. po::options_description desc(\"Allowed options\"); // Add the standard options FW::Options::addCommonOptions<po::options_description>(desc); // Map to store the given program options po::variables_map vm; // Get all options from contain line and store it into the map po::store(po::parse_command_line(argc, argv, desc), vm); po::notify(vm); // Print help if reqested if (vm.count(\"help\")) { std::cout << desc << std::endl; return 1; } // Read the common options auto nEvents = FW::Options::readNumberOfEvents<po::variables_map>(vm); auto logLevel = FW::Options::readLogLevel<po::variables_map>(vm); // And add the hello world algorithm std::shared_ptr<FW::IAlgorithm> hWorld( new FWE::HelloWorldAlgorithm(logLevel)); // Create the config object for the sequencer FW::Sequencer::Config seqConfig; // Now create the sequencer FW::Sequencer sequencer(seqConfig); sequencer.appendEventAlgorithms({hWorld}); sequencer.run(nEvents); // Return 0 for success return 0; } Let us first execute the binary with the --help option which prints out the available options for this executable. Allowed options: --help Produce help message -n [ --events ] arg (=1) The number of events to be processed -l [ --loglevel ] arg (=2) The output log level. Please set the wished number (0 = VERBOSE, 1 = DEBUG, 2 = INFO, 3 = WARNING, 4 = ERROR, 5 = FATAL). Only the number of events and the output log level can be changed for the HelloWorld example, which when executed with default parameters yields acts-tester$ ./ACTFWHelloWorldExample 14:38:38 Sequencer INFO Appended algorithm HelloWorld 14:38:38 Sequencer INFO Starting event loop for 14:38:38 Sequencer INFO 0 services 14:38:38 Sequencer INFO 0 readers 14:38:38 Sequencer INFO 0 writers 14:38:38 Sequencer INFO 1 algorithms 14:38:38 Sequencer INFO Run the event loop 14:38:38 Sequencer INFO start event 0 14:38:38 HelloWorld INFO Hello World! (from event 0) 14:38:38 Sequencer INFO event 0 done 14:38:38 Sequencer INFO Running end-of-run hooks of writers and services One can now modify the number of events and the screen output level, e.g. acts-tester$ ./ACTFWHelloWorldExample -l0 14:39:37 Sequencer INFO Appended algorithm HelloWorld 14:39:37 Sequencer INFO Starting event loop for 14:39:37 Sequencer INFO 0 services 14:39:37 Sequencer INFO 0 readers 14:39:37 Sequencer INFO 0 writers 14:39:37 Sequencer INFO 1 algorithms 14:39:37 Sequencer INFO Run the event loop 14:39:37 Sequencer INFO start event 0 14:39:37 HelloWorld INFO Hello World! (from event 0) 14:39:37 HelloWorld DEBUG - that is an ACTS_DEBUG message 14:39:37 HelloWorld VERBOSE - that is an ACTS_VERBOSE message 14:39:37 Sequencer INFO event 0 done 14:39:37 Sequencer INFO Running end-of-run hooks of writers and services or (with export ACTSFW_NUM_THREADS=1 ): acts-tester$ ./ACTFWHelloWorldExample -n5 14:41:15 Sequencer INFO Appended algorithm HelloWorld 14:41:15 Sequencer INFO Starting event loop for 14:41:15 Sequencer INFO 0 services 14:41:15 Sequencer INFO 0 readers 14:41:15 Sequencer INFO 0 writers 14:41:15 Sequencer INFO 1 algorithms 14:41:15 Sequencer INFO Run the event loop 14:41:15 Sequencer INFO start event 0 14:41:15 HelloWorld INFO Hello World! (from event 0) 14:41:15 Sequencer INFO event 0 done 14:41:15 Sequencer INFO start event 1 14:41:15 HelloWorld INFO Hello World! (from event 1) 14:41:15 Sequencer INFO event 1 done 14:41:15 Sequencer INFO start event 2 14:41:15 HelloWorld INFO Hello World! (from event 2) 14:41:15 Sequencer INFO event 2 done 14:41:15 Sequencer INFO start event 3 14:41:15 HelloWorld INFO Hello World! (from event 3) 14:41:15 Sequencer INFO event 3 done 14:41:15 Sequencer INFO start event 4 14:41:15 HelloWorld INFO Hello World! (from event 4) 14:41:15 Sequencer INFO event 4 done 14:41:15 Sequencer INFO Running end-of-run hooks of writers and service When increasing the available number of threads, one can see how the events get executed in parallel and the screen print out gets scrambled: acts-tester$ ./ACTFWHelloWorldExample -n5 14:43:23 Sequencer INFO Appended algorithm HelloWorld 14:43:23 Sequencer INFO Starting event loop for 14:43:23 Sequencer INFO 0 services 14:43:23 Sequencer INFO 0 readers 14:43:23 Sequencer INFO 0 writers 14:43:23 Sequencer INFO 1 algorithms 14:43:23 Sequencer INFO Run the event loop 14:43:23 Sequencer INFO start event 2 14:43:23 Sequencer INFO start event 3 14:43:23 Sequencer INFO start event 0 14:43:23 HelloWorld INFO Hello World! (from event 2) 14:43:23 HelloWorld INFO Hello World! (from event 3) 14:43:23 Sequencer INFO start event 4 14:43:23 Sequencer INFO event 3 done 14:43:23 HelloWorld INFO Hello World! (from event 4) 14:43:23 Sequencer INFO start event 1 14:43:23 HelloWorld INFO Hello World! (from event 0) 14:43:23 Sequencer INFO event 2 done 14:43:23 Sequencer INFO event 4 done 14:43:23 Sequencer INFO event 0 done 14:43:23 HelloWorld INFO Hello World! (from event 1) 14:43:23 Sequencer INFO event 1 done 14:43:23 Sequencer INFO Running end-of-run hooks of writers and services","title":"Hello World"},{"location":"guides_framework/#geometry-building","text":"This example does not run an event loop, it only build the geometry and evokes writers for the output of the geometry into some visualisation format ( .obj ). There are different ways to build an Acts geometry, depending on which input geometry is chosen: GenericGeometry, i.e. an adhoc geometry descrbed in Detectors/GenericGeometry using an explicit C++ description RootGeometry, a TGeo/ROOT based geometry constructed with the TGeoLayerBuilder from acts-core DD4hepGeometry, a TGeo/DD4hep based geometry constructed with the DD4hepLayerBuilder from acts-core Let us start with the ACTFWGenericGeometryExample , and inspect the options: ./ACTFWGenericGeometryExample --help Allowed options: --help Produce help message -n [ --events ] arg (=1) The number of events to be processed -l [ --loglevel ] arg (=2) The output log level. Please set the wished number (0 = VERBOSE, 1 = DEBUG, 2 = INFO, 3 = WARNING, 4 = ERROR, 5 = FATAL). --obj-tg-fileheader arg The (optional) file header for the tracking geometry. --obj-tg-sensitiveheader arg The (optional) header in front of sensitive sensors. --obj-tg-layerheader arg The (optional) header in front of layer surfaces. --obj-sf-fileheader arg The (optional) file header for the surface writer. --obj-sf-phisegments arg (=72) Number of phi segments to approximate curves. --obj-sf-outputPrecission arg (=6) Floating number output precission. --obj-sf-outputScalor arg (=1) Scale factor to be applied. --obj-sf-outputThickness arg (=1) The surface thickness. --obj-sf-outputSensitive arg (=1) Write sensitive surfaces. --obj-sf-outputLayers arg (=1) Write layer surfaces. --geo-surface-loglevel arg (=3) The outoput log level for the surface building. --geo-layer-loglevel arg (=3) The output log level for the layer building. --geo-volume-loglevel arg (=3) The output log level for the volume building. --geo-subdetectors arg (= ) Sub detectors for the output writing --geo-material-mode arg (=1) Modes are: 0 (none), 1 (construct), 2 (load), 3 (proto) --geo-material-file arg (=material-maps.root) File for the material maps to be loaded. --output-dir arg Output directory location. --output-root arg (=0) Switch on to write '.root' output file(s). --output-csv arg (=0) Switch on to write '.csv' output file(s). --output-obj arg (=0) Switch on to write '.obj' ouput file(s). --output-json arg (=0) Switch on to write '.json' ouput file(s). There are different steering options concering the output, most importantly the --geo-subdetectors option that reads the list of sub detectors which should be written out for this detector. The standard GenericGeometry is built from a beam pipe, a pixel detector, a pixel support tube, short strips detector (SStrip) and long strings (LStrip), hence we can evoke the following call. ./ACTFWGenericGeometryExample --output-obj 1 --geo-subdetectors BeamPipe Pixel PST SStrip LStrip This creates dedicated obj files that can be used for displaying the detector with a standard viewer that can read the obj format: -rw-r--r-- 1 salzburg staff 13542 Jan 15 16:59 BeamPipe.obj -rw-r--r-- 1 salzburg staff 3095801 Jan 15 16:59 LStrip.obj -rw-r--r-- 1 salzburg staff 13580 Jan 15 16:59 PST.obj -rw-r--r-- 1 salzburg staff 1963033 Jan 15 16:59 Pixel.obj -rw-r--r-- 1 salzburg staff 3543361 Jan 15 16:59 SStrip.obj The following is an example of Pixel.obj displayed with a standard online obj viewer With appropriate input, the ACTFWRootGeometryExample and DD4hepGeometryExample are to be used in a very similar way.","title":"Geometry Building"},{"location":"guides_framework/#propagation-example","text":"The PropagationExample again exists for the different geoemtry backends and is based on the Algorithms/PropagationAlgorithm module. It performs random test propagations throught he detector and allows to write them with certain debug information into appropriate writers. Let us continue with the Generic geometry example, and first inspect he options: next to the geometry building options ( --geo-xxx-yyy ) there appear options for the magnetic field setup ( --bf-xxx-yyy ) and for the behaviour of the propagation ( --prop-xx-yy ). Running actstest$ ./ACTFWGenericPropagationExample -n 10 --prop-ntests 10000 --bf-values 0 0 2 --output-root 1 Performs 10 events of each 10000 test propagations through a magnetic field of 2 Tesla in z direction and produces a root output file (by default propagation-steps.root ) with the default EigenStepper (Runge-Kutta-Nystrom integration). Inspecting the root files reveals the detector and its interaction with the runge kutta propagation algorithm, the following pictures shows all steps the stepper has performed, either due to navigation instruction, accuracy control, abort condition and user setting: Restricting the steps to only those performed for navigational purposes, shows: And, furthermore, requiring that sensitive material was hit, reveils: Which leaves the steps that are necessary due to accuracy control to be: One can clearly see how lower momentum tracks need more steps.","title":"Propagation Example"},{"location":"guides_framework/#material-mapping-and-material-validation","text":"The Acts toolkit consists of an automated system to map a complex detector geometry onto a more suitable, simplified geometry model for reconstruction.","title":"Material Mapping and Material Validation"},{"location":"guides_framework/#material-mapping","text":"The MaterialMapping example demonstrates how to map and project material information from a complex detector geometry onto a set of chosen layers. As an input to the MaterialMapping algorithm a collection of recorded material information is needed. The input format for the mapper is a collection of MaterialInteraction objects, this is usually generated with Geant4 and an appropriate G4UserAction can be found in the acts-framework/Plugins/Geant4 plugin. Alternatively, the MaterialInteractor of the acts-core module can also write out such a collection. For the purpose of this example, we use the ad-hoc built material description of the GenericDetector , which has certain material properties on sensitive elements and support structure, such as on the beam pipe and the PST. To create an input collection, one just needs to run the ACTFWGenericMaterialValidationExample and write out the recorded material with the root output option. ./ACTFWGenericMaterialValidationExample -n 1000 --prop-ntests 1000 --prop-energyloss 0 --prop-scattering 0 --prop-record-material 1 --output-root 1 This executes 1000 events with each 1000 straight line tracks using the PropagationAlgorithm with energy loss and scattering switched off, but configured to record material and output a root file (default: propagation-material.root ). Let's inspect the root file, by plotting all position in a central region where material was found One can clearly see the support structures and the sensitive surface carrying material. We can also look at the total amount of material in this tracker, here shown in thickness in terms of radiation length: Now, let us use this input file to map it onto a simpler configuration. The generic detector is designed to do so. When chosing the option --geo-material-mode=3 , the exact same detector will be built, but instead of a material descriptionb for sensitive surfaces and support structures, only certain layers are loaded with a so called ProtoSurfaceMaterial object, which is a placeholder for the mapping. It consists of a binning prescription, i.e. a directive how granular one wants the maps on this layer to be done. Let us now run the mapping process with the more complicated input from the GenericDetector and map it onto a few surfaces with a dedicated binning structure. ./ACTFWGenericMaterialMappingExample -n 1000000 --geo-material-mode=3 --output-root 1 --input-root 1 --input-files propagation-material.root From our prior validation job of 1000 events times 1000 tests we have 1000000 mapping events available. ATTENTION: Since the mapping process combines information from all events into a single collection of material maps, this has to be run in single trheaded mode, i.e. with export ACTSFW_NUM_THREADS=1 . At the end of this job, some screen information shows which maps have been produced: 15:18:11 Sequencer INFO Running end-of-run hooks of writers and services 15:18:11 IndexedMater INFO Writing out map at Material_vol5_lay2_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay2_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay4_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay6_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay8_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay10_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay12_app0_sen0 15:18:11 IndexedMater INFO Writing out map at Material_vol7_lay14_app0_sen0 And, in this configuration, a root file containing the maps is produced: -rw-r--r-- 1 salzburg staff 1430046 Jan 16 15:18 material-maps.root We can have a look at those maps and even plot them: One can see, how the module structure with its overlaps in now projected into one binned material map.","title":"Material Mapping"},{"location":"guides_framework/#material-validation","text":"The MaterialValidation example is built upon the PropagationAlgorithm : it runs test propagations and records the material inforamtion in the tracker. If configured with the --root-output 1 option, this iformation is written into a root file. It has already be used in the MaterialMapping example to produce the material information that is then later mapped onto the ProtoSurfaceMaterial to produce the material maps to be loaded. After having produces such material maps, we can now create the geometry with loaded maps and compare the material budget to the orginal material source. ./ACTFWGenericMaterialValidationExample -n 100 --prop-ntests 1000 --prop-energyloss 0 --prop-scattering 0 --prop-record-material 1 --output-root 1 --geo-material-mode 2 --geo-material-file=material-maps.root The options --geo-material-mode 2 and --geo-material-file=material-maps.root steer hereby to load the material back from a file and do not construct it ad-hoc. Let us compare the material recorded with the original detector (black) and the maps (red), first the positions: One can perfectly see how the original map was more detailed and complicated and how the maps concentrate the material on simplified surfaces. However, when comparing the total material budget in the detector, only little compromise has been done and due to the binning structure of the maps, the structure in both pseudorapidity and azimuthal angle is pretty much preserved: This result can improved by optimising the binning structure.","title":"Material Validation"},{"location":"guides_testing/","text":"Acts tests and continuous integration Modern software development techniques relies on automated tests and continuous integration platforms to ensure the functionality of a given software package. It verifies that existing behaviour remains unchanged when updating or optimizing its implementation and allows automated testing of new functionality. Acts provides an extensive unit test suite that checks geometry and propagation functionality at the smallest unit level, e.g. a single geometric surface type. Tests are based on known solutions for simple cases or manual calculations for selected configuration. These tests can be run very fast and usually take less that a second. They allow fast iteration and verification for the developers. In addition, we have started to provide larger integration tests that aim to test combinations of multiple Acts components. A typical example would be a full geometry propagation chain. It includes geometry description, geometry navigation, and particle propagation that all have to work together to allow efficient transport of particles through a tracking detector. The integration tests usually check for self-consistency or variations from previous known-good results for a large variety of inputs. They are expected to have a run time of the order of minutes and are intended to run as part of a merge request. Acts employs the continuous integration platform provided by the CERN Gitlab installation. Within this platform each change pushed to the common repository is compiled with multiple build configurations. The build configuration are chosen to encompass all supported Linux distributions (Scientific Linux CERN 6, CERN CentOS 7), different compilers (GCC, CLang) and different LCG software releases starting from the minimum requirement (LCG93). Additional checks are performed to ensure consistent licensing and formatting. The available platforms were chosen to be consistent with typical setups available to users, e.g. on lxplus machines at CERN. Special care was taken to not require any additional software but the ones provided by the distributions themselves and the LCG releases with the versions that are provided. The continuous integration builds activate all optional components to avoid unintentional breakage due to deactivated components on a developers local copy. Together ensures consistent functionality and ease-of-use on all platforms and greatly reduces possible barriers for future users.","title":"CI and Testing Guide"},{"location":"guides_testing/#acts-tests-and-continuous-integration","text":"Modern software development techniques relies on automated tests and continuous integration platforms to ensure the functionality of a given software package. It verifies that existing behaviour remains unchanged when updating or optimizing its implementation and allows automated testing of new functionality. Acts provides an extensive unit test suite that checks geometry and propagation functionality at the smallest unit level, e.g. a single geometric surface type. Tests are based on known solutions for simple cases or manual calculations for selected configuration. These tests can be run very fast and usually take less that a second. They allow fast iteration and verification for the developers. In addition, we have started to provide larger integration tests that aim to test combinations of multiple Acts components. A typical example would be a full geometry propagation chain. It includes geometry description, geometry navigation, and particle propagation that all have to work together to allow efficient transport of particles through a tracking detector. The integration tests usually check for self-consistency or variations from previous known-good results for a large variety of inputs. They are expected to have a run time of the order of minutes and are intended to run as part of a merge request. Acts employs the continuous integration platform provided by the CERN Gitlab installation. Within this platform each change pushed to the common repository is compiled with multiple build configurations. The build configuration are chosen to encompass all supported Linux distributions (Scientific Linux CERN 6, CERN CentOS 7), different compilers (GCC, CLang) and different LCG software releases starting from the minimum requirement (LCG93). Additional checks are performed to ensure consistent licensing and formatting. The available platforms were chosen to be consistent with typical setups available to users, e.g. on lxplus machines at CERN. Special care was taken to not require any additional software but the ones provided by the distributions themselves and the LCG releases with the versions that are provided. The continuous integration builds activate all optional components to avoid unintentional breakage due to deactivated components on a developers local copy. Together ensures consistent functionality and ease-of-use on all platforms and greatly reduces possible barriers for future users.","title":"Acts tests and continuous integration"},{"location":"guides_users/","text":"Getting started Getting started Prerequisites Installation CMake build system Build Acts on lxplus Build Acts on your local machine Using Acts in your own cmake project Documentation Prerequisites Only few dependencies are required to build the Core library of Acts. A list of prerequisites required is given below with the version numbers indicating which versions were tested. Older versions may or may not work, feedback is very welcome. The following dependencies are required: A C++14 compatible compiler, e.g. gcc (>= 6.2) or clang (>= 4.0) cmake (>= 3.7) boost (>= 1.62, with program_options and unit_test_framework ) Eigen (>= 3.2.9) The following dependencies are optional and are only needed for some of the components: DD4Hep (>= 1.02) for the DD4Hep plugin doxygen (>= 1.8.11) for the documentation graphviz (>= 2.28.00) for the documentation ROOT (>= 6.10.00) for the TGeo plugin Compatible versions of all dependencies are provided by LCG releases . The current recommended release for building Acts is the LCG94 Release . This release is also used in the continous integration (CI) system to test the software. Setup scripts are provided in the repository that can be used to setup this release, and a few others, on lxplus machines at CERN (see below ). Installation and quick start The Acts repository is hosted on the GitLab instance at CERN. In order to aquire the latest version from the git repository you can simply clone: git clone https://gitlab.cern.ch/acts/acts-core.git <ACTS_DIR> You can then cd <ACTS_DIR> continue building Acts: source CI/setup_lcg94.sh mkdir build && cd build cmake .. cmake --build . -- install CMake build system CMake is used as build system for compiling and installing Acts. For a complete list of CMake options please refer to the official documentation and this nice list of general cmake options . Important options relevant for the Acts project are given below. They are set by adding -D<OPTION>=<VALUE> to the cmake command. option default description ACTS_BUILD_DD4HEP_PLUGIN OFF Build DD4HEP geometry plugin ACTS_BUILD_DIGITIZATION_PLUGIN OFF Build Digitization plugin ACTS_BUILD_JSON_PLUGIN OFF Build Json plugin for geometry input/output ACTS_BUILD_IDENTIFICATION_PLUGIN OFF Build Identification plugin ACTS_BUILD_TGEO_PLUGIN OFF Build TGeo geometry plugin ACTS_BUILD_FATRAS OFF Build FAst TRAcking Simulation package ACTS_BUILD_LEGACY OFF Build Legacy package ACTS_BUILD_BENCHMARKS OFF Build benchmarks ACTS_BUILD_EXAMPLES OFF Build examples ACTS_BUILD_UNITTESTS OFF Build unit tests ACTS_BUILD_INTEGRATIONTESTS OFF Build integration tests ACTS_BUILD_DOC OFF Build documentation ACTS_USE_BUNDLED_NLOHMANN_JSON ON Use external or bundled Json library CMAKE_INSTALL_PREFIX The installation directory CMAKE_PREFIX_PATH Search path for external packages CMAKE_CXX_COMPILER Set C++ compiler (e.g. g++ or clang++) CMAKE_BUILD_TYPE Build type (e.g. Debug, Release) affects compiler flags DD4hep_DIR Path to the DD4hep installation Building Acts Building Acts on lxplus The first step to build Acts is to acquire supported versions of the dependencies. Which dependencies are required depends on the plugins you enable, as mentioned above. If you are in an lxplus-like environment (i.e. SLC6 or CC7 , with cvmfs ), you can use the setup scripts located in <ACTS_DIR>/CI to get the dependencies: source <ACTS_DIR>/CI/setup_lcgXYZ.sh where XYZ is the version number of the LCG release. There are multiple setup scripts for different LCG releases, which corresponds to the releases the CI tests against. The releases which can be set up using these scripts are therefore sure to be compatible. You can build Acts with any of these releases . Additionally, there is a script called setup_clang.sh which will make the clang compiler available on top of one of the LCG releases. This configuration is also tested by the CI. Using one of the scripts, you can use the following commands to build Acts with all plugins using the same dependency versions as in the continous integration system. source CI/setup_lcg94.sh # example, you can use any of the provided scripts. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=<path you want> \\ -DACTS_BUILD_DD4HEP_PLUGIN=ON \\ -DACTS_BUILD_TGEO_PLUGIN=ON .. cmake --build . -- install In this example the DD4hep, Material and TGeo plugins. The install prefix is set to <path you want> . Building Acts on your local machine Building and running Acts on your local machine is not offically supported. However, if you have the necessary prerequisites installed it should be possible to use it locally. Acts developers regularly use different recent Linux distributions and macOS to build and develop Acts. Using Acts in your own cmake project When using Acts in your own cmake-based project, you need to include the following lines in your CMakeLists.txt file: find_package (Acts COMPONENTS comp1 comp2 ...) where compX are the required components from the Acts project. See the cmake output for more information about which components are available. Documentation You can find a complete documentation of the Acts functionality and the class reference guide at http://acts.web.cern.ch/ACTS/latest/doc/index.html .","title":"Users Guide"},{"location":"guides_users/#getting-started","text":"Getting started Prerequisites Installation CMake build system Build Acts on lxplus Build Acts on your local machine Using Acts in your own cmake project Documentation","title":"Getting started"},{"location":"guides_users/#prerequisites","text":"Only few dependencies are required to build the Core library of Acts. A list of prerequisites required is given below with the version numbers indicating which versions were tested. Older versions may or may not work, feedback is very welcome. The following dependencies are required: A C++14 compatible compiler, e.g. gcc (>= 6.2) or clang (>= 4.0) cmake (>= 3.7) boost (>= 1.62, with program_options and unit_test_framework ) Eigen (>= 3.2.9) The following dependencies are optional and are only needed for some of the components: DD4Hep (>= 1.02) for the DD4Hep plugin doxygen (>= 1.8.11) for the documentation graphviz (>= 2.28.00) for the documentation ROOT (>= 6.10.00) for the TGeo plugin Compatible versions of all dependencies are provided by LCG releases . The current recommended release for building Acts is the LCG94 Release . This release is also used in the continous integration (CI) system to test the software. Setup scripts are provided in the repository that can be used to setup this release, and a few others, on lxplus machines at CERN (see below ).","title":"Prerequisites"},{"location":"guides_users/#installation-and-quick-start","text":"The Acts repository is hosted on the GitLab instance at CERN. In order to aquire the latest version from the git repository you can simply clone: git clone https://gitlab.cern.ch/acts/acts-core.git <ACTS_DIR> You can then cd <ACTS_DIR> continue building Acts: source CI/setup_lcg94.sh mkdir build && cd build cmake .. cmake --build . -- install","title":"Installation and quick start"},{"location":"guides_users/#cmake-build-system","text":"CMake is used as build system for compiling and installing Acts. For a complete list of CMake options please refer to the official documentation and this nice list of general cmake options . Important options relevant for the Acts project are given below. They are set by adding -D<OPTION>=<VALUE> to the cmake command. option default description ACTS_BUILD_DD4HEP_PLUGIN OFF Build DD4HEP geometry plugin ACTS_BUILD_DIGITIZATION_PLUGIN OFF Build Digitization plugin ACTS_BUILD_JSON_PLUGIN OFF Build Json plugin for geometry input/output ACTS_BUILD_IDENTIFICATION_PLUGIN OFF Build Identification plugin ACTS_BUILD_TGEO_PLUGIN OFF Build TGeo geometry plugin ACTS_BUILD_FATRAS OFF Build FAst TRAcking Simulation package ACTS_BUILD_LEGACY OFF Build Legacy package ACTS_BUILD_BENCHMARKS OFF Build benchmarks ACTS_BUILD_EXAMPLES OFF Build examples ACTS_BUILD_UNITTESTS OFF Build unit tests ACTS_BUILD_INTEGRATIONTESTS OFF Build integration tests ACTS_BUILD_DOC OFF Build documentation ACTS_USE_BUNDLED_NLOHMANN_JSON ON Use external or bundled Json library CMAKE_INSTALL_PREFIX The installation directory CMAKE_PREFIX_PATH Search path for external packages CMAKE_CXX_COMPILER Set C++ compiler (e.g. g++ or clang++) CMAKE_BUILD_TYPE Build type (e.g. Debug, Release) affects compiler flags DD4hep_DIR Path to the DD4hep installation","title":"CMake build system"},{"location":"guides_users/#building-acts","text":"","title":"Building Acts"},{"location":"guides_users/#building-acts-on-lxplus","text":"The first step to build Acts is to acquire supported versions of the dependencies. Which dependencies are required depends on the plugins you enable, as mentioned above. If you are in an lxplus-like environment (i.e. SLC6 or CC7 , with cvmfs ), you can use the setup scripts located in <ACTS_DIR>/CI to get the dependencies: source <ACTS_DIR>/CI/setup_lcgXYZ.sh where XYZ is the version number of the LCG release. There are multiple setup scripts for different LCG releases, which corresponds to the releases the CI tests against. The releases which can be set up using these scripts are therefore sure to be compatible. You can build Acts with any of these releases . Additionally, there is a script called setup_clang.sh which will make the clang compiler available on top of one of the LCG releases. This configuration is also tested by the CI. Using one of the scripts, you can use the following commands to build Acts with all plugins using the same dependency versions as in the continous integration system. source CI/setup_lcg94.sh # example, you can use any of the provided scripts. mkdir build && cd build cmake -DCMAKE_INSTALL_PREFIX=<path you want> \\ -DACTS_BUILD_DD4HEP_PLUGIN=ON \\ -DACTS_BUILD_TGEO_PLUGIN=ON .. cmake --build . -- install In this example the DD4hep, Material and TGeo plugins. The install prefix is set to <path you want> .","title":"Building Acts on lxplus"},{"location":"guides_users/#building-acts-on-your-local-machine","text":"Building and running Acts on your local machine is not offically supported. However, if you have the necessary prerequisites installed it should be possible to use it locally. Acts developers regularly use different recent Linux distributions and macOS to build and develop Acts.","title":"Building Acts on your local machine"},{"location":"guides_users/#using-acts-in-your-own-cmake-project","text":"When using Acts in your own cmake-based project, you need to include the following lines in your CMakeLists.txt file: find_package (Acts COMPONENTS comp1 comp2 ...) where compX are the required components from the Acts project. See the cmake output for more information about which components are available.","title":"Using Acts in your own cmake project"},{"location":"guides_users/#documentation","text":"You can find a complete documentation of the Acts functionality and the class reference guide at http://acts.web.cern.ch/ACTS/latest/doc/index.html .","title":"Documentation"},{"location":"integration/","text":"Integration Technical integration In order to use Acts in your project, you need to integrate it in your build. Currently, only CMake-based builds are supported. Customizing Acts Acts can be customized in several ways in order to allow seamless integration in the experiment software. Screen output logging A default screen logging system is integrated in Acts, but can be replaced with the experiment's logging system. Plugins at compile time Several implementation details of Acts can be reconfigured at compile time. These are header-only files that exchange the default implementations of Acts. Change of parameter definition This can be done by providing the different parameter definition file and declaring it as ACTS_PARAMETER_DEFINITIONS_PLUGIN='<path_to>/<filename.hpp>' TODO: Write a CI test for this Change of Identifier class The Identifier class can be exchanged by declaring: ACTS_CORE_IDENTIFIER_PLUGIN TODO: Write a CI test for this, together with Identifier change Interfacing with magnetic field The magnetic field integration is done using a concept implementation that is designed for field cell caching. The propagation modules take the magnetic field as a template argument, hence providing the correct concept guarantess compatibility with Acts. System of Units Acts uses a set of system of units and its interanl code tries to minimise unit conversions, however, when converting into the Acts data model, this should be done using the Acts units Internally, Acts uses a set of SI2Nat<> template funtion to free the code from conversion constants. /// @brief physical quantities for selecting right conversion function enum Quantity { MOMENTUM, ENERGY, LENGTH, MASS }; /// @cond template <Quantity> double SI2Nat(const double); template <Quantity> double Nat2SI(const double); TODO: Write a unit test for this, investigate if we can do this as a PLUGIN as well Configuration Acts does not come with a dedicated configuration system to bind to any experiments software. Instead, a simple decision has been taken to equip every configurable component of Acts with a nested Config struct. This struct object is used to construct the corresponding Acts tool, e.g. class CylinderVolumeBuilder : public ITrackingVolumeBuilder { public: /// @struct Config /// Nested configuration struct for this CylinderVolumeBuilder struct Config { /// the trackign volume helper for construction std::shared_ptr<const ITrackingVolumeHelper> trackingVolumeHelper = nullptr; /// the string based indenfication std::string volumeName = \"\"; /// The dimensions of the manually created world std::vector<double> volumeDimension = {}; /// the world material std::shared_ptr<const Material> volumeMaterial = nullptr; /// build the volume to the beam line bool buildToRadiusZero = false; /// needed to build layers within the volume std::shared_ptr<const ILayerBuilder> layerBuilder = nullptr; /// the envelope covering the potential layers rMin, rMax std::pair<double, double> layerEnvelopeR = {1. * Acts::units::_mm, 1. * Acts::units::_mm}; /// the envelope covering the potential layers inner/outer double layerEnvelopeZ = 10. * Acts::units::_mm; /// the volume signature int volumeSignature = -1; }; }; To configure Acts tools, thus, one has to guarantee that necessary configuration parameters are forwarded through the framework configuration mechanism.","title":"Integration"},{"location":"integration/#integration","text":"","title":"Integration"},{"location":"integration/#technical-integration","text":"In order to use Acts in your project, you need to integrate it in your build. Currently, only CMake-based builds are supported.","title":"Technical integration"},{"location":"integration/#customizing-acts","text":"Acts can be customized in several ways in order to allow seamless integration in the experiment software.","title":"Customizing Acts"},{"location":"integration/#screen-output-logging","text":"A default screen logging system is integrated in Acts, but can be replaced with the experiment's logging system.","title":"Screen output logging"},{"location":"integration/#plugins-at-compile-time","text":"Several implementation details of Acts can be reconfigured at compile time. These are header-only files that exchange the default implementations of Acts.","title":"Plugins at compile time"},{"location":"integration/#change-of-parameter-definition","text":"This can be done by providing the different parameter definition file and declaring it as ACTS_PARAMETER_DEFINITIONS_PLUGIN='<path_to>/<filename.hpp>' TODO: Write a CI test for this","title":"Change of parameter definition"},{"location":"integration/#change-of-identifier-class","text":"The Identifier class can be exchanged by declaring: ACTS_CORE_IDENTIFIER_PLUGIN TODO: Write a CI test for this, together with Identifier change","title":"Change of Identifier class"},{"location":"integration/#interfacing-with-magnetic-field","text":"The magnetic field integration is done using a concept implementation that is designed for field cell caching. The propagation modules take the magnetic field as a template argument, hence providing the correct concept guarantess compatibility with Acts.","title":"Interfacing with magnetic field"},{"location":"integration/#system-of-units","text":"Acts uses a set of system of units and its interanl code tries to minimise unit conversions, however, when converting into the Acts data model, this should be done using the Acts units Internally, Acts uses a set of SI2Nat<> template funtion to free the code from conversion constants. /// @brief physical quantities for selecting right conversion function enum Quantity { MOMENTUM, ENERGY, LENGTH, MASS }; /// @cond template <Quantity> double SI2Nat(const double); template <Quantity> double Nat2SI(const double); TODO: Write a unit test for this, investigate if we can do this as a PLUGIN as well","title":"System of Units"},{"location":"integration/#configuration","text":"Acts does not come with a dedicated configuration system to bind to any experiments software. Instead, a simple decision has been taken to equip every configurable component of Acts with a nested Config struct. This struct object is used to construct the corresponding Acts tool, e.g. class CylinderVolumeBuilder : public ITrackingVolumeBuilder { public: /// @struct Config /// Nested configuration struct for this CylinderVolumeBuilder struct Config { /// the trackign volume helper for construction std::shared_ptr<const ITrackingVolumeHelper> trackingVolumeHelper = nullptr; /// the string based indenfication std::string volumeName = \"\"; /// The dimensions of the manually created world std::vector<double> volumeDimension = {}; /// the world material std::shared_ptr<const Material> volumeMaterial = nullptr; /// build the volume to the beam line bool buildToRadiusZero = false; /// needed to build layers within the volume std::shared_ptr<const ILayerBuilder> layerBuilder = nullptr; /// the envelope covering the potential layers rMin, rMax std::pair<double, double> layerEnvelopeR = {1. * Acts::units::_mm, 1. * Acts::units::_mm}; /// the envelope covering the potential layers inner/outer double layerEnvelopeZ = 10. * Acts::units::_mm; /// the volume signature int volumeSignature = -1; }; }; To configure Acts tools, thus, one has to guarantee that necessary configuration parameters are forwarded through the framework configuration mechanism.","title":"Configuration"},{"location":"integration_atlas/","text":"Using Acts from Athena First steps have been taken towards use Acts inside Athena . The current goal is to demonstrate that the Acts geometry design can accomodate current and future ATLAS tracking geometries without requiring dedicated modifications in the library. This means building an Acts::TrackingGeometry , which contains a hierarchy of TrackingVolume s. These, in turn, encompass layers of detector elements. Having done this, Acts can be used to propagate particles through the geometry. This allows the testing of the navigation that is required to follow a particle trajectory through the detector. Since Acts is designed to be configurable, the construction of the layer structure is delegated to a LayerBuilder utility object. A LayerBuilder object, that is aware of Athena and the way the detector geometry is acessible within it, can provide Acts with all the information that is needed to construct the tracking geometry. Acts::SurfaceArrayCreator::Config sacCfg; sacCfg.surfaceMatcher = matcher; // <- allows injection of Identifier info auto surfaceArrayCreator = std::make_shared<Acts::SurfaceArrayCreator>( sacCfg, Acts::getDefaultLogger(\"SurfaceArrayCreator\", Acts::Logging::VERBOSE)); // ... GeoModelLayerBuilder::Config cfg; // layer builder gets the detector manager cfg.mng = static_cast<const InDetDD::SiDetectorManager*>(manager); gmLayerBuilder = std::make_shared<const GMLB>(cfg, Acts::getDefaultLogger(\"GMLayBldr\", Acts::Logging::VERBOSE)); Acts::CylinderVolumeBuilder::Config cvbConfig; cvbConfig.layerEnvelopeR = {0, 0}; cvbConfig.layerEnvelopeZ = 2; cvbConfig.trackingVolumeHelper = cvh; // ... cvbConfig.layerBuilder = gmLayerBuilder; Acts::TrackingGeometryBuilder::Config tgbConfig; tgbConfig.trackingVolumeHelper = cylinderVolumeHelper; tgbConfig.trackingVolumeBuilders = volumeBuilders; auto trackingGeometryBuilder = std::make_shared<const Acts::TrackingGeometryBuilder>(tgbConfig); std::shared_ptr<const Acts::TrackingGeometry> trackingGeometry = trackingGeometryBuilder->trackingGeometry(); Tracking surfaces in Acts are coupled to a detector element object, which needs to fulfill a certain interface. A dedicated detector element class, which can wrap an ATLAS detector element object, was written. At this point, the class tries to abstract away some differences between detector elements from the different ATLAS subdetectors, which are irrelevant to Acts and allow the LayerBuilder to be less verbose. The detector identification helpers are an example for this. The detector element needs to provide a transform which describes the transition into the local reference frame of it's active surface. In this case, the transform is stored as a pointer and returned as a const reference when requested. This should in principle allow for changes of the transforms transparently to Acts. This would be helpful as an initial approach to support handling of alignment, since no dedicated infrastructure for alignment is in place within Acts. Current ATLAS Inner Detector The detector managers allow access to all the detector elements. The layer builder loops over all elements and wraps them into a Acts detector element. Subsequently, the identifiers attached to the detector elements allow figuring out which detector part and layer the elements belong to. The elements are thus collected into buckets for each layer in each component. The elements are then handed over to the LayerCreator utility, which is part of the Acts core, and is versatile enough to handle ATLAS geometries. Identifier information can be injected into the LayerCreator , which allows determining layer size and binning automatically. In the case of the TRT, the layer builder currently uses the TRT_Numerology to figure out the number of layers without looping over elements. The detector manager provides a method which allows access to detector elements based on the indices returned by TRT_Numerology . As of now, every straw is translated into a detector element in Acts. The endcaps are built as 160 separate disc layers containing one layer of straws. In the barrel, there is one layer per barrel ring. The binning is not optimal at this point, an arbitrary \\(\\phi\\) binning should work reasonably well. ITk With some modifications, the layer builder can build the ITk geometry in 20.20 . One adjustment is the fact that the meaning of the identifier parts eta_module and layer_disk changed for the Pixel endcaps. In the ID, layer_disk enumerates the endcap themselves, while eta_module enumerates the rings in \\(r\\) . In 20.20 , eta_module enumerates the \\(z\\) -position of rings, while layer_disk enumerates the rings in \\(r\\) . A problem arises here, because eta_module now only enumerates rings present in a given eta_module . This way, the identifier cannot be used which elements belong to the same layer (in the Acts sense). Design Currently, the tracking geometry is built from inside one algorithm. The particle propagation loop also runs within the algorithm. This will need to be reworked, so that access to the Acts tracking geometry is available in a reusable way.","title":"Integration in ATLAS"},{"location":"integration_atlas/#using-acts-from-athena","text":"First steps have been taken towards use Acts inside Athena . The current goal is to demonstrate that the Acts geometry design can accomodate current and future ATLAS tracking geometries without requiring dedicated modifications in the library. This means building an Acts::TrackingGeometry , which contains a hierarchy of TrackingVolume s. These, in turn, encompass layers of detector elements. Having done this, Acts can be used to propagate particles through the geometry. This allows the testing of the navigation that is required to follow a particle trajectory through the detector. Since Acts is designed to be configurable, the construction of the layer structure is delegated to a LayerBuilder utility object. A LayerBuilder object, that is aware of Athena and the way the detector geometry is acessible within it, can provide Acts with all the information that is needed to construct the tracking geometry. Acts::SurfaceArrayCreator::Config sacCfg; sacCfg.surfaceMatcher = matcher; // <- allows injection of Identifier info auto surfaceArrayCreator = std::make_shared<Acts::SurfaceArrayCreator>( sacCfg, Acts::getDefaultLogger(\"SurfaceArrayCreator\", Acts::Logging::VERBOSE)); // ... GeoModelLayerBuilder::Config cfg; // layer builder gets the detector manager cfg.mng = static_cast<const InDetDD::SiDetectorManager*>(manager); gmLayerBuilder = std::make_shared<const GMLB>(cfg, Acts::getDefaultLogger(\"GMLayBldr\", Acts::Logging::VERBOSE)); Acts::CylinderVolumeBuilder::Config cvbConfig; cvbConfig.layerEnvelopeR = {0, 0}; cvbConfig.layerEnvelopeZ = 2; cvbConfig.trackingVolumeHelper = cvh; // ... cvbConfig.layerBuilder = gmLayerBuilder; Acts::TrackingGeometryBuilder::Config tgbConfig; tgbConfig.trackingVolumeHelper = cylinderVolumeHelper; tgbConfig.trackingVolumeBuilders = volumeBuilders; auto trackingGeometryBuilder = std::make_shared<const Acts::TrackingGeometryBuilder>(tgbConfig); std::shared_ptr<const Acts::TrackingGeometry> trackingGeometry = trackingGeometryBuilder->trackingGeometry(); Tracking surfaces in Acts are coupled to a detector element object, which needs to fulfill a certain interface. A dedicated detector element class, which can wrap an ATLAS detector element object, was written. At this point, the class tries to abstract away some differences between detector elements from the different ATLAS subdetectors, which are irrelevant to Acts and allow the LayerBuilder to be less verbose. The detector identification helpers are an example for this. The detector element needs to provide a transform which describes the transition into the local reference frame of it's active surface. In this case, the transform is stored as a pointer and returned as a const reference when requested. This should in principle allow for changes of the transforms transparently to Acts. This would be helpful as an initial approach to support handling of alignment, since no dedicated infrastructure for alignment is in place within Acts.","title":"Using Acts from Athena"},{"location":"integration_atlas/#current-atlas-inner-detector","text":"The detector managers allow access to all the detector elements. The layer builder loops over all elements and wraps them into a Acts detector element. Subsequently, the identifiers attached to the detector elements allow figuring out which detector part and layer the elements belong to. The elements are thus collected into buckets for each layer in each component. The elements are then handed over to the LayerCreator utility, which is part of the Acts core, and is versatile enough to handle ATLAS geometries. Identifier information can be injected into the LayerCreator , which allows determining layer size and binning automatically. In the case of the TRT, the layer builder currently uses the TRT_Numerology to figure out the number of layers without looping over elements. The detector manager provides a method which allows access to detector elements based on the indices returned by TRT_Numerology . As of now, every straw is translated into a detector element in Acts. The endcaps are built as 160 separate disc layers containing one layer of straws. In the barrel, there is one layer per barrel ring. The binning is not optimal at this point, an arbitrary \\(\\phi\\) binning should work reasonably well.","title":"Current ATLAS Inner Detector"},{"location":"integration_atlas/#itk","text":"With some modifications, the layer builder can build the ITk geometry in 20.20 . One adjustment is the fact that the meaning of the identifier parts eta_module and layer_disk changed for the Pixel endcaps. In the ID, layer_disk enumerates the endcap themselves, while eta_module enumerates the rings in \\(r\\) . In 20.20 , eta_module enumerates the \\(z\\) -position of rings, while layer_disk enumerates the rings in \\(r\\) . A problem arises here, because eta_module now only enumerates rings present in a given eta_module . This way, the identifier cannot be used which elements belong to the same layer (in the Acts sense).","title":"ITk"},{"location":"integration_atlas/#design","text":"Currently, the tracking geometry is built from inside one algorithm. The particle propagation loop also runs within the algorithm. This will need to be reworked, so that access to the Acts tracking geometry is available in a reusable way.","title":"Design"},{"location":"integration_fcc/","text":"Usage for the Future Circular Collider study Acts is used as tracking toolkit for the Future Circular Collider (FCC) design study. The FCC Software suite FCCSW uses Gaudi as an event processing framework, DD4hep (Detector Description for High Energy Physics) for the geometry description and Geant4 as a simulation package. This chapter describes the currently ongoing integration and usage of Acts in FCCSW. Acts makes use of a plugin mechanism to allow interfacing experiment software where necessary, e.g. geometry, identification, event data model, magnetic field. In the core part Acts reduces the dependencies to a minimum using configuration structs which can be interfaced by the user. For more information please see the general integration chapter. Integration of Acts into FCCSW Gaudi Services, Tools and Algorithms are used to interface to Acts. They either use provided functionality directly or act as wrapper internally holding an instance of the Acts Object. Using the python job option the Acts tools can be configured (using their configuration structs) by the user at runtime: In FCCSW the tracking toolkit is not only used for track fitting but has various applications. For instance the magnetic field service of FCCSW is based on the Acts implementation. Another example is the application of fast simulation using the extrapolation through the tracking geometry. Since for the FCChh conceptual design study no specific detector technologies are selected yet, Acts is used to perform geometric digitization . Forwarding logging messages to Gaudi As explained here , the Acts logging messages can be forwarded to the Gaudi message service during an event by overloading the default Acts logging implementation. In the following one can see, for example, overload of the Acts print policy (definition how and where to print): Geometry Translation In FCCSW we require to have one common source of detector description for all applications, including the differnt types of simulation and reconstruction. In order to allow fast reconstruction Acts internally uses a simplfied tracking geometry. For automatic and consistent geometry translation from DD4hep the plugin mechanism was used and a DD4hepPlugin established. The DD4hepPlugin provides a convenience function handing back the world volume of the Acts tracking geometry from the DD4hep detector. Inside FCCSW a tracking geometry service was established which calls the function and hands back the Acts tracking geometry. The sensitive surfaces in the tracking geometry have a direct link to the underlying detector element of Acts, which allows to handle conditions data and alignment. Magnetic Field implementation As explained here , Acts is agnostic to the magnetic field implementation, as long as it follows the given magnetic field concept . For convenience Acts provides already two different magentic field implementations which are being used inside FCCSW. Firstly a configurable constant magnetic field service and an interpolated magnetic field service which linearly interpolates the magnetic field within cells of a given grid. To stay independent from the file format Acts provides convenience methods to facilitate creating the grid from std vectors of grid points. Reading in the values from the actual file (e.g. root, txt/csv) happens inside FCCSW. The two configurable FCC magnetic field service implementations (constant and interpolated) hold the dedicated Acts magnetic field implementation as a member and forward the calls to it: The FCChh magnetic field map which acts as input for the FCC interpolated magnetic service can be easily configured using the gaudi job option file: Extrapolation The extrapolation through the tracking geometry is used during reconstruction. A second application of the extrapolation through the tracking geometry is for fast simulation. In order to allow both applications to use the extrapolation with different configuration a Gaudi Tool which holds an instance to the Acts extrapolation was created. This tool can be configured differently for both applications at runtime: The extrapolation tool can then be used by an Gaudi algorithm which handles the translations from and to the FCC edm. For example in the ExtrapolationTest below the reads in generated particles from the event store and after extrapolating through the tracking geometry, translates the output into FCC track hits: Geometric Digitization Because the specific detector technologies which will be used for the future hadron hadron collider are not known yet the Acts geometric digitzation tools are being implemented for FCChh. Every detector element inside Acts holds a pointer to a DigitizationModule . The digitization module has information about readout relevant information e.g. segmentation, readout drift direction, lorentz angle. The DD4hepPlugin enables either automatic translation from the given readout information from DD4hep during the conversion or the possibility that the user can append the digitization module. The first version creates one digitzation module for every sensitive surface which is very expensive in CPU. Since many sensitive detector elements will have the same readout segmentation, the second variation allows the user to once create a shared instance of a digitization module and append it to many mdoules. Convenience functions which hand back the Acts digitization module from a given dd4hep readout have been created. The Acts geometric digitization determines the cells hit by a particle given the hit position and the momentum direction. Afterwards the clusters are created from the pixels using a connected components analysis algorithm from boost. The user can decide if pixels sharing a common corner or a common edge should be merged. Using the Acts digitzation tools one can emulate digital readout as well as analogue readout which smears the energy deposit in the cells. Below one can see how the GeometricTrackerDigitizer , which is currently being developed inside FCCSW and uses the Acts digitization tools, can be used in the python job options. It reads in hits ( digiTrackHitAssociation ) produced by FCC geant4 full simulation and writes out trackClusters to the FCC event store:","title":"Integration in FCCSW"},{"location":"integration_fcc/#usage-for-the-future-circular-collider-study","text":"Acts is used as tracking toolkit for the Future Circular Collider (FCC) design study. The FCC Software suite FCCSW uses Gaudi as an event processing framework, DD4hep (Detector Description for High Energy Physics) for the geometry description and Geant4 as a simulation package. This chapter describes the currently ongoing integration and usage of Acts in FCCSW. Acts makes use of a plugin mechanism to allow interfacing experiment software where necessary, e.g. geometry, identification, event data model, magnetic field. In the core part Acts reduces the dependencies to a minimum using configuration structs which can be interfaced by the user. For more information please see the general integration chapter.","title":"Usage for the Future Circular Collider study"},{"location":"integration_fcc/#integration-of-acts-into-fccsw","text":"Gaudi Services, Tools and Algorithms are used to interface to Acts. They either use provided functionality directly or act as wrapper internally holding an instance of the Acts Object. Using the python job option the Acts tools can be configured (using their configuration structs) by the user at runtime: In FCCSW the tracking toolkit is not only used for track fitting but has various applications. For instance the magnetic field service of FCCSW is based on the Acts implementation. Another example is the application of fast simulation using the extrapolation through the tracking geometry. Since for the FCChh conceptual design study no specific detector technologies are selected yet, Acts is used to perform geometric digitization .","title":"Integration of Acts into FCCSW"},{"location":"integration_fcc/#forwarding-logging-messages-to-gaudi","text":"As explained here , the Acts logging messages can be forwarded to the Gaudi message service during an event by overloading the default Acts logging implementation. In the following one can see, for example, overload of the Acts print policy (definition how and where to print):","title":"Forwarding logging messages to Gaudi"},{"location":"integration_fcc/#geometry-translation","text":"In FCCSW we require to have one common source of detector description for all applications, including the differnt types of simulation and reconstruction. In order to allow fast reconstruction Acts internally uses a simplfied tracking geometry. For automatic and consistent geometry translation from DD4hep the plugin mechanism was used and a DD4hepPlugin established. The DD4hepPlugin provides a convenience function handing back the world volume of the Acts tracking geometry from the DD4hep detector. Inside FCCSW a tracking geometry service was established which calls the function and hands back the Acts tracking geometry. The sensitive surfaces in the tracking geometry have a direct link to the underlying detector element of Acts, which allows to handle conditions data and alignment.","title":"Geometry Translation"},{"location":"integration_fcc/#magnetic-field-implementation","text":"As explained here , Acts is agnostic to the magnetic field implementation, as long as it follows the given magnetic field concept . For convenience Acts provides already two different magentic field implementations which are being used inside FCCSW. Firstly a configurable constant magnetic field service and an interpolated magnetic field service which linearly interpolates the magnetic field within cells of a given grid. To stay independent from the file format Acts provides convenience methods to facilitate creating the grid from std vectors of grid points. Reading in the values from the actual file (e.g. root, txt/csv) happens inside FCCSW. The two configurable FCC magnetic field service implementations (constant and interpolated) hold the dedicated Acts magnetic field implementation as a member and forward the calls to it: The FCChh magnetic field map which acts as input for the FCC interpolated magnetic service can be easily configured using the gaudi job option file:","title":"Magnetic Field implementation"},{"location":"integration_fcc/#extrapolation","text":"The extrapolation through the tracking geometry is used during reconstruction. A second application of the extrapolation through the tracking geometry is for fast simulation. In order to allow both applications to use the extrapolation with different configuration a Gaudi Tool which holds an instance to the Acts extrapolation was created. This tool can be configured differently for both applications at runtime: The extrapolation tool can then be used by an Gaudi algorithm which handles the translations from and to the FCC edm. For example in the ExtrapolationTest below the reads in generated particles from the event store and after extrapolating through the tracking geometry, translates the output into FCC track hits:","title":"Extrapolation"},{"location":"integration_fcc/#geometric-digitization","text":"Because the specific detector technologies which will be used for the future hadron hadron collider are not known yet the Acts geometric digitzation tools are being implemented for FCChh. Every detector element inside Acts holds a pointer to a DigitizationModule . The digitization module has information about readout relevant information e.g. segmentation, readout drift direction, lorentz angle. The DD4hepPlugin enables either automatic translation from the given readout information from DD4hep during the conversion or the possibility that the user can append the digitization module. The first version creates one digitzation module for every sensitive surface which is very expensive in CPU. Since many sensitive detector elements will have the same readout segmentation, the second variation allows the user to once create a shared instance of a digitization module and append it to many mdoules. Convenience functions which hand back the Acts digitization module from a given dd4hep readout have been created. The Acts geometric digitization determines the cells hit by a particle given the hit position and the momentum direction. Afterwards the clusters are created from the pixels using a connected components analysis algorithm from boost. The user can decide if pixels sharing a common corner or a common edge should be merged. Using the Acts digitzation tools one can emulate digital readout as well as analogue readout which smears the energy deposit in the cells. Below one can see how the GeometricTrackerDigitizer , which is currently being developed inside FCCSW and uses the Acts digitization tools, can be used in the python job options. It reads in hits ( digiTrackHitAssociation ) produced by FCC geant4 full simulation and writes out trackClusters to the FCC event store:","title":"Geometric Digitization"},{"location":"integration_trackml/","text":"","title":"Tracking Machine Learning Challenge"},{"location":"license/","text":"License Acts is licensed under the Mozilla Public License Version 2.0. The full text of the license can be found here or below. Mozilla Public License Version 2.0 ================================== 1. Definitions -------------- 1.1. \"Contributor\" means each individual or legal entity that creates, contributes to the creation of, or owns Covered Software. 1.2. \"Contributor Version\" means the combination of the Contributions of others (if any) used by a Contributor and that particular Contributor's Contribution. 1.3. \"Contribution\" means Covered Software of a particular Contributor. 1.4. \"Covered Software\" means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof. 1.5. \"Incompatible With Secondary Licenses\" means (a) that the initial Contributor has attached the notice described in Exhibit B to the Covered Software; or (b) that the Covered Software was made available under the terms of version 1.1 or earlier of the License, but not also under the terms of a Secondary License. 1.6. \"Executable Form\" means any form of the work other than Source Code Form. 1.7. \"Larger Work\" means a work that combines Covered Software with other material, in a separate file or files, that is not Covered Software. 1.8. \"License\" means this document. 1.9. \"Licensable\" means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently, any and all of the rights conveyed by this License. 1.10. \"Modifications\" means any of the following: (a) any file in Source Code Form that results from an addition to, deletion from, or modification of the contents of Covered Software; or (b) any new file in Source Code Form that contains any Covered Software. 1.11. \"Patent Claims\" of a Contributor means any patent claim(s), including without limitation, method, process, and apparatus claims, in any patent Licensable by such Contributor that would be infringed, but for the grant of the License, by the making, using, selling, offering for sale, having made, import, or transfer of either its Contributions or its Contributor Version. 1.12. \"Secondary License\" means either the GNU General Public License, Version 2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General Public License, Version 3.0, or any later versions of those licenses. 1.13. \"Source Code Form\" means the form of the work preferred for making modifications. 1.14. \"You\" (or \"Your\") means an individual or a legal entity exercising rights under this License. For legal entities, \"You\" includes any entity that controls, is controlled by, or is under common control with You. For purposes of this definition, \"control\" means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity. 2. License Grants and Conditions -------------------------------- 2.1. Grants Each Contributor hereby grants You a world-wide, royalty-free, non-exclusive license: (a) under intellectual property rights (other than patent or trademark) Licensable by such Contributor to use, reproduce, make available, modify, display, perform, distribute, and otherwise exploit its Contributions, either on an unmodified basis, with Modifications, or as part of a Larger Work; and (b) under Patent Claims of such Contributor to make, use, sell, offer for sale, have made, import, and otherwise transfer either its Contributions or its Contributor Version. 2.2. Effective Date The licenses granted in Section 2.1 with respect to any Contribution become effective for each Contribution on the date the Contributor first distributes such Contribution. 2.3. Limitations on Grant Scope The licenses granted in this Section 2 are the only rights granted under this License. No additional rights or licenses will be implied from the distribution or licensing of Covered Software under this License. Notwithstanding Section 2.1(b) above, no patent license is granted by a Contributor: (a) for any code that a Contributor has removed from Covered Software; or (b) for infringements caused by: (i) Your and any other third party's modifications of Covered Software, or (ii) the combination of its Contributions with other software (except as part of its Contributor Version); or (c) under Patent Claims infringed by Covered Software in the absence of its Contributions. This License does not grant any rights in the trademarks, service marks, or logos of any Contributor (except as may be necessary to comply with the notice requirements in Section 3.4). 2.4. Subsequent Licenses No Contributor makes additional grants as a result of Your choice to distribute the Covered Software under a subsequent version of this License (see Section 10.2) or under the terms of a Secondary License (if permitted under the terms of Section 3.3). 2.5. Representation Each Contributor represents that the Contributor believes its Contributions are its original creation(s) or it has sufficient rights to grant the rights to its Contributions conveyed by this License. 2.6. Fair Use This License is not intended to limit any rights You have under applicable copyright doctrines of fair use, fair dealing, or other equivalents. 2.7. Conditions Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in Section 2.1. 3. Responsibilities ------------------- 3.1. Distribution of Source Form All distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients' rights in the Source Code Form. 3.2. Distribution of Executable Form If You distribute Covered Software in Executable Form then: (a) such Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and (b) You may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients' rights in the Source Code Form under this License. 3.3. Distribution of a Larger Work You may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. If the Larger Work is a combination of Covered Software with a work governed by one or more Secondary Licenses, and the Covered Software is not Incompatible With Secondary Licenses, this License permits You to additionally distribute such Covered Software under the terms of such Secondary License(s), so that the recipient of the Larger Work may, at their option, further distribute the Covered Software under the terms of either this License or such Secondary License(s). 3.4. Notices You may not remove or alter the substance of any license notices (including copyright notices, patent notices, disclaimers of warranty, or limitations of liability) contained within the Source Code Form of the Covered Software, except that You may alter any license notices to the extent required to remedy known factual inaccuracies. 3.5. Application of Additional Terms You may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, You may do so only on Your own behalf, and not on behalf of any Contributor. You must make it absolutely clear that any such warranty, support, indemnity, or liability obligation is offered by You alone, and You hereby agree to indemnify every Contributor for any liability incurred by such Contributor as a result of warranty, support, indemnity or liability terms You offer. You may include additional disclaimers of warranty and limitations of liability specific to any jurisdiction. 4. Inability to Comply Due to Statute or Regulation --------------------------------------------------- If it is impossible for You to comply with any of the terms of this License with respect to some or all of the Covered Software due to statute, judicial order, or regulation then You must: (a) comply with the terms of this License to the maximum extent possible; and (b) describe the limitations and the code they affect. Such description must be placed in a text file included with all distributions of the Covered Software under this License. Except to the extent prohibited by statute or regulation, such description must be sufficiently detailed for a recipient of ordinary skill to be able to understand it. 5. Termination -------------- 5.1. The rights granted under this License will terminate automatically if You fail to comply with any of its terms. However, if You become compliant, then the rights granted under this License from a particular Contributor are reinstated (a) provisionally, unless and until such Contributor explicitly and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor fails to notify You of the non-compliance by some reasonable means prior to 60 days after You have come back into compliance. Moreover, Your grants from a particular Contributor are reinstated on an ongoing basis if such Contributor notifies You of the non-compliance by some reasonable means, this is the first time You have received notice of non-compliance with this License from such Contributor, and You become compliant prior to 30 days after Your receipt of the notice. 5.2. If You initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a Contributor Version directly or indirectly infringes any patent, then the rights granted to You by any and all Contributors for the Covered Software under Section 2.1 of this License shall terminate. 5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user license agreements (excluding distributors and resellers) which have been validly granted by You or Your distributors under this License prior to termination shall survive termination. ************************************************************************ * * * 6. Disclaimer of Warranty * * ------------------------- * * * * Covered Software is provided under this License on an \"as is\" * * basis, without warranty of any kind, either expressed, implied, or * * statutory, including, without limitation, warranties that the * * Covered Software is free of defects, merchantable, fit for a * * particular purpose or non-infringing. The entire risk as to the * * quality and performance of the Covered Software is with You. * * Should any Covered Software prove defective in any respect, You * * (not any Contributor) assume the cost of any necessary servicing, * * repair, or correction. This disclaimer of warranty constitutes an * * essential part of this License. No use of any Covered Software is * * authorized under this License except under this disclaimer. * * * ************************************************************************ ************************************************************************ * * * 7. Limitation of Liability * * -------------------------- * * * * Under no circumstances and under no legal theory, whether tort * * (including negligence), contract, or otherwise, shall any * * Contributor, or anyone who distributes Covered Software as * * permitted above, be liable to You for any direct, indirect, * * special, incidental, or consequential damages of any character * * including, without limitation, damages for lost profits, loss of * * goodwill, work stoppage, computer failure or malfunction, or any * * and all other commercial damages or losses, even if such party * * shall have been informed of the possibility of such damages. This * * limitation of liability shall not apply to liability for death or * * personal injury resulting from such party's negligence to the * * extent applicable law prohibits such limitation. Some * * jurisdictions do not allow the exclusion or limitation of * * incidental or consequential damages, so this exclusion and * * limitation may not apply to You. * * * ************************************************************************ 8. Litigation ------------- Any litigation relating to this License may be brought only in the courts of a jurisdiction where the defendant maintains its principal place of business and such litigation shall be governed by laws of that jurisdiction, without reference to its conflict-of-law provisions. Nothing in this Section shall prevent a party's ability to bring cross-claims or counter-claims. 9. Miscellaneous ---------------- This License represents the complete agreement concerning the subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not be used to construe this License against a Contributor. 10. Versions of the License --------------------------- 10.1. New Versions Mozilla Foundation is the license steward. Except as provided in Section 10.3, no one other than the license steward has the right to modify or publish new versions of this License. Each version will be given a distinguishing version number. 10.2. Effect of New Versions You may distribute the Covered Software under the terms of the version of the License under which You originally received the Covered Software, or under the terms of any subsequent version published by the license steward. 10.3. Modified Versions If you create software not governed by this License, and you want to create a new license for such software, you may create and use a modified version of this License if you rename the license and remove any references to the name of the license steward (except to note that such modified license differs from this License). 10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses If You choose to distribute Source Code Form that is Incompatible With Secondary Licenses under the terms of this version of the License, the notice described in Exhibit B of this License must be attached. Exhibit A - Source Code Form License Notice ------------------------------------------- This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership. Exhibit B - \"Incompatible With Secondary Licenses\" Notice --------------------------------------------------------- This Source Code Form is \"Incompatible With Secondary Licenses\", as defined by the Mozilla Public License, v. 2.0.","title":"License"},{"location":"license/#license","text":"Acts is licensed under the Mozilla Public License Version 2.0. The full text of the license can be found here or below. Mozilla Public License Version 2.0 ================================== 1. Definitions -------------- 1.1. \"Contributor\" means each individual or legal entity that creates, contributes to the creation of, or owns Covered Software. 1.2. \"Contributor Version\" means the combination of the Contributions of others (if any) used by a Contributor and that particular Contributor's Contribution. 1.3. \"Contribution\" means Covered Software of a particular Contributor. 1.4. \"Covered Software\" means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof. 1.5. \"Incompatible With Secondary Licenses\" means (a) that the initial Contributor has attached the notice described in Exhibit B to the Covered Software; or (b) that the Covered Software was made available under the terms of version 1.1 or earlier of the License, but not also under the terms of a Secondary License. 1.6. \"Executable Form\" means any form of the work other than Source Code Form. 1.7. \"Larger Work\" means a work that combines Covered Software with other material, in a separate file or files, that is not Covered Software. 1.8. \"License\" means this document. 1.9. \"Licensable\" means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently, any and all of the rights conveyed by this License. 1.10. \"Modifications\" means any of the following: (a) any file in Source Code Form that results from an addition to, deletion from, or modification of the contents of Covered Software; or (b) any new file in Source Code Form that contains any Covered Software. 1.11. \"Patent Claims\" of a Contributor means any patent claim(s), including without limitation, method, process, and apparatus claims, in any patent Licensable by such Contributor that would be infringed, but for the grant of the License, by the making, using, selling, offering for sale, having made, import, or transfer of either its Contributions or its Contributor Version. 1.12. \"Secondary License\" means either the GNU General Public License, Version 2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General Public License, Version 3.0, or any later versions of those licenses. 1.13. \"Source Code Form\" means the form of the work preferred for making modifications. 1.14. \"You\" (or \"Your\") means an individual or a legal entity exercising rights under this License. For legal entities, \"You\" includes any entity that controls, is controlled by, or is under common control with You. For purposes of this definition, \"control\" means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity. 2. License Grants and Conditions -------------------------------- 2.1. Grants Each Contributor hereby grants You a world-wide, royalty-free, non-exclusive license: (a) under intellectual property rights (other than patent or trademark) Licensable by such Contributor to use, reproduce, make available, modify, display, perform, distribute, and otherwise exploit its Contributions, either on an unmodified basis, with Modifications, or as part of a Larger Work; and (b) under Patent Claims of such Contributor to make, use, sell, offer for sale, have made, import, and otherwise transfer either its Contributions or its Contributor Version. 2.2. Effective Date The licenses granted in Section 2.1 with respect to any Contribution become effective for each Contribution on the date the Contributor first distributes such Contribution. 2.3. Limitations on Grant Scope The licenses granted in this Section 2 are the only rights granted under this License. No additional rights or licenses will be implied from the distribution or licensing of Covered Software under this License. Notwithstanding Section 2.1(b) above, no patent license is granted by a Contributor: (a) for any code that a Contributor has removed from Covered Software; or (b) for infringements caused by: (i) Your and any other third party's modifications of Covered Software, or (ii) the combination of its Contributions with other software (except as part of its Contributor Version); or (c) under Patent Claims infringed by Covered Software in the absence of its Contributions. This License does not grant any rights in the trademarks, service marks, or logos of any Contributor (except as may be necessary to comply with the notice requirements in Section 3.4). 2.4. Subsequent Licenses No Contributor makes additional grants as a result of Your choice to distribute the Covered Software under a subsequent version of this License (see Section 10.2) or under the terms of a Secondary License (if permitted under the terms of Section 3.3). 2.5. Representation Each Contributor represents that the Contributor believes its Contributions are its original creation(s) or it has sufficient rights to grant the rights to its Contributions conveyed by this License. 2.6. Fair Use This License is not intended to limit any rights You have under applicable copyright doctrines of fair use, fair dealing, or other equivalents. 2.7. Conditions Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in Section 2.1. 3. Responsibilities ------------------- 3.1. Distribution of Source Form All distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients' rights in the Source Code Form. 3.2. Distribution of Executable Form If You distribute Covered Software in Executable Form then: (a) such Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and (b) You may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients' rights in the Source Code Form under this License. 3.3. Distribution of a Larger Work You may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. If the Larger Work is a combination of Covered Software with a work governed by one or more Secondary Licenses, and the Covered Software is not Incompatible With Secondary Licenses, this License permits You to additionally distribute such Covered Software under the terms of such Secondary License(s), so that the recipient of the Larger Work may, at their option, further distribute the Covered Software under the terms of either this License or such Secondary License(s). 3.4. Notices You may not remove or alter the substance of any license notices (including copyright notices, patent notices, disclaimers of warranty, or limitations of liability) contained within the Source Code Form of the Covered Software, except that You may alter any license notices to the extent required to remedy known factual inaccuracies. 3.5. Application of Additional Terms You may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, You may do so only on Your own behalf, and not on behalf of any Contributor. You must make it absolutely clear that any such warranty, support, indemnity, or liability obligation is offered by You alone, and You hereby agree to indemnify every Contributor for any liability incurred by such Contributor as a result of warranty, support, indemnity or liability terms You offer. You may include additional disclaimers of warranty and limitations of liability specific to any jurisdiction. 4. Inability to Comply Due to Statute or Regulation --------------------------------------------------- If it is impossible for You to comply with any of the terms of this License with respect to some or all of the Covered Software due to statute, judicial order, or regulation then You must: (a) comply with the terms of this License to the maximum extent possible; and (b) describe the limitations and the code they affect. Such description must be placed in a text file included with all distributions of the Covered Software under this License. Except to the extent prohibited by statute or regulation, such description must be sufficiently detailed for a recipient of ordinary skill to be able to understand it. 5. Termination -------------- 5.1. The rights granted under this License will terminate automatically if You fail to comply with any of its terms. However, if You become compliant, then the rights granted under this License from a particular Contributor are reinstated (a) provisionally, unless and until such Contributor explicitly and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor fails to notify You of the non-compliance by some reasonable means prior to 60 days after You have come back into compliance. Moreover, Your grants from a particular Contributor are reinstated on an ongoing basis if such Contributor notifies You of the non-compliance by some reasonable means, this is the first time You have received notice of non-compliance with this License from such Contributor, and You become compliant prior to 30 days after Your receipt of the notice. 5.2. If You initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a Contributor Version directly or indirectly infringes any patent, then the rights granted to You by any and all Contributors for the Covered Software under Section 2.1 of this License shall terminate. 5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user license agreements (excluding distributors and resellers) which have been validly granted by You or Your distributors under this License prior to termination shall survive termination. ************************************************************************ * * * 6. Disclaimer of Warranty * * ------------------------- * * * * Covered Software is provided under this License on an \"as is\" * * basis, without warranty of any kind, either expressed, implied, or * * statutory, including, without limitation, warranties that the * * Covered Software is free of defects, merchantable, fit for a * * particular purpose or non-infringing. The entire risk as to the * * quality and performance of the Covered Software is with You. * * Should any Covered Software prove defective in any respect, You * * (not any Contributor) assume the cost of any necessary servicing, * * repair, or correction. This disclaimer of warranty constitutes an * * essential part of this License. No use of any Covered Software is * * authorized under this License except under this disclaimer. * * * ************************************************************************ ************************************************************************ * * * 7. Limitation of Liability * * -------------------------- * * * * Under no circumstances and under no legal theory, whether tort * * (including negligence), contract, or otherwise, shall any * * Contributor, or anyone who distributes Covered Software as * * permitted above, be liable to You for any direct, indirect, * * special, incidental, or consequential damages of any character * * including, without limitation, damages for lost profits, loss of * * goodwill, work stoppage, computer failure or malfunction, or any * * and all other commercial damages or losses, even if such party * * shall have been informed of the possibility of such damages. This * * limitation of liability shall not apply to liability for death or * * personal injury resulting from such party's negligence to the * * extent applicable law prohibits such limitation. Some * * jurisdictions do not allow the exclusion or limitation of * * incidental or consequential damages, so this exclusion and * * limitation may not apply to You. * * * ************************************************************************ 8. Litigation ------------- Any litigation relating to this License may be brought only in the courts of a jurisdiction where the defendant maintains its principal place of business and such litigation shall be governed by laws of that jurisdiction, without reference to its conflict-of-law provisions. Nothing in this Section shall prevent a party's ability to bring cross-claims or counter-claims. 9. Miscellaneous ---------------- This License represents the complete agreement concerning the subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not be used to construe this License against a Contributor. 10. Versions of the License --------------------------- 10.1. New Versions Mozilla Foundation is the license steward. Except as provided in Section 10.3, no one other than the license steward has the right to modify or publish new versions of this License. Each version will be given a distinguishing version number. 10.2. Effect of New Versions You may distribute the Covered Software under the terms of the version of the License under which You originally received the Covered Software, or under the terms of any subsequent version published by the license steward. 10.3. Modified Versions If you create software not governed by this License, and you want to create a new license for such software, you may create and use a modified version of this License if you rename the license and remove any references to the name of the license steward (except to note that such modified license differs from this License). 10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses If You choose to distribute Source Code Form that is Incompatible With Secondary Licenses under the terms of this version of the License, the notice described in Exhibit B of this License must be attached. Exhibit A - Source Code Form License Notice ------------------------------------------- This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/. If it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice. You may add additional accurate notices of copyright ownership. Exhibit B - \"Incompatible With Secondary Licenses\" Notice --------------------------------------------------------- This Source Code Form is \"Incompatible With Secondary Licenses\", as defined by the Mozilla Public License, v. 2.0.","title":"License"},{"location":"modules_eventdata/","text":"Event Data Track parameterisation A trajectory in a magnetic field is generally parameterised by a set of at least five parameters (when being bound to a surface). Two different categories are used in Acts: so-called bound parameters, i.e. parameter bound to a surface and curvilinear parameters. Curvilinear parameters are defined in an implicit planar and normal (to the track) reference plane that follows the track. The center of the implicitely defined plane is at the current track position, while the normal vector points along the momentum direction. Per definition the local parameters of a curvilinear parameterisation are thus fixed to (0,0). In Acts the parameterisation is can be changed, provided the according transformations into global coordinates are given, it can be changed by adapting the relevant coordinate_transformation definition. This shows an excerpt of the default implementation: namespace Acts { namespace detail { /** * @brief helper structure summarizing coordinate transformations */ struct coordinate_transformation { typedef ActsVector<ParValue_t, Acts::NGlobalPars> ParVector_t; static ActsVectorD<3> parameters2globalPosition(const ParVector_t& pars, const Surface& s) { ActsVectorD<3> globalPosition; s.localToGlobal(ActsVectorD<2>(pars(Acts::eLOC_0), pars(Acts::eLOC_1)), parameters2globalMomentum(pars), globalPosition); return globalPosition; } static ActsVectorD<3> parameters2globalMomentum(const ParVector_t& pars) { ActsVectorD<3> momentum; double p = std::abs(1. / pars(Acts::eQOP)); double phi = pars(Acts::ePHI); double theta = pars(Acts::eTHETA); momentum << p * sin(theta) * cos(phi), p * sin(theta) * sin(phi), p * cos(theta); return momentum; } static ParVector_t global2curvilinear(const ActsVectorD<3>&, const ActsVectorD<3>& mom, double charge) { ParVector_t parameters; parameters << 0, 0, mom.phi(), mom.theta(), ((std::abs(charge) < 1e-4) ? 1. : charge) / mom.mag(); return parameters; } // ... }; } // end of namespace detail } // end of namespace Acts The default parameterisation is chosen to be the ATLAS parameterisation: namespace Acts { enum ParDef : unsigned int { eLOC_0 = 0, ///< first coordinate in local surface frame eLOC_1 = 1, ///< second coordinate in local surface frame eLOC_R = eLOC_0, eLOC_PHI = eLOC_1, eLOC_RPHI = eLOC_0, eLOC_Z = eLOC_1, eLOC_X = eLOC_0, eLOC_Y = eLOC_1, eLOC_D0 = eLOC_0, eLOC_Z0 = eLOC_1, ePHI = 2, ///< phi direction of momentum in global frame eTHETA = 3, ///< theta direction of momentum in global frame eQOP = 4, ///< charge/momentum for charged tracks, for neutral tracks it is /// 1/momentum NGlobalPars }; Changing the default parameter definition and transformation needs recompilation of the Acts Software and redefinition of the relevant Plugin: target_compile_definitions (Acts::Core PUBLIC -DACTS_PARAMETER_DEFINITIONS_PLUGIN=\"${ACTS_PARAMETER_DEFINITIONS_PLUGIN}\") Neutral and charged representations Track parameters come in a charged and neutral flavor, the flavor is defined using a template parameter, which either is a ChargedPolicy class for charged track parameter representation: namespace Acts { typedef SingleTrackParameters<ChargedPolicy> TrackParameters; typedef SingleCurvilinearTrackParameters<ChargedPolicy> CurvilinearParameters; typedef SingleBoundTrackParameters<ChargedPolicy> BoundParameters; } // end of namespace Acts Or, respectively, a NeutralPolicy object namespace Acts { typedef SingleTrackParameters<NeutralPolicy> NeutralParameters; typedef SingleCurvilinearTrackParameters<NeutralPolicy> NeutralCurvilinearParameters; typedef SingleBoundTrackParameters<NeutralPolicy> NeutralBoundParameters; } // end of namespace Acts Multivariant representation Multivariant fitters, such as the Gaussian Sum filter rely on a multi-component description of the track, which requires the definition of multivariant track paramters. For Acts, as Propagator and Extrapolator are written for type templates, a multivariant definition of track parameterisation is planned in order to integrate directly with the core software. Measurements Measurements exist as uncalibrated and calibrated types. While uncalibrated measurements are the direct output of the data formation, during the track fit, when trajectory information (or information about the trajectory hypothesis) is available, certain calibration steps can help to improve the track fit quality. Uncalibrated measurements Measurements in a detector can be one to many-dimensional (covering the full range up to the full track parameterisation). template <typename Identifier, ParID_t... params> class Measurement { // check type conditions static_assert(std::is_copy_constructible<Identifier>::value, \"'Identifier' must be copy-constructible\"); static_assert(std::is_move_constructible<Identifier>::value, \"'Identifier' must be move-constructible\"); static_assert(std::is_copy_assignable<Identifier>::value, \"'Identifier' must be copy-assignable\"); static_assert(std::is_move_assignable<Identifier>::value, \"'Identifier' must be move-assignable\"); private: // private typedef's typedef ParameterSet<params...> ParSet_t; ///< type of the underlying ParameterSet object // ... }; In order to minimise the computational cost (and differently from the original ATLAS code base), the dimension of the measurement has to be fixed at compile time. Calibrated measurements Calibrated measurements are temporary objects needed for track fitting and belonging to a track. Still to be defined","title":"Event Data"},{"location":"modules_eventdata/#event-data","text":"","title":"Event Data"},{"location":"modules_eventdata/#track-parameterisation","text":"A trajectory in a magnetic field is generally parameterised by a set of at least five parameters (when being bound to a surface). Two different categories are used in Acts: so-called bound parameters, i.e. parameter bound to a surface and curvilinear parameters. Curvilinear parameters are defined in an implicit planar and normal (to the track) reference plane that follows the track. The center of the implicitely defined plane is at the current track position, while the normal vector points along the momentum direction. Per definition the local parameters of a curvilinear parameterisation are thus fixed to (0,0). In Acts the parameterisation is can be changed, provided the according transformations into global coordinates are given, it can be changed by adapting the relevant coordinate_transformation definition. This shows an excerpt of the default implementation: namespace Acts { namespace detail { /** * @brief helper structure summarizing coordinate transformations */ struct coordinate_transformation { typedef ActsVector<ParValue_t, Acts::NGlobalPars> ParVector_t; static ActsVectorD<3> parameters2globalPosition(const ParVector_t& pars, const Surface& s) { ActsVectorD<3> globalPosition; s.localToGlobal(ActsVectorD<2>(pars(Acts::eLOC_0), pars(Acts::eLOC_1)), parameters2globalMomentum(pars), globalPosition); return globalPosition; } static ActsVectorD<3> parameters2globalMomentum(const ParVector_t& pars) { ActsVectorD<3> momentum; double p = std::abs(1. / pars(Acts::eQOP)); double phi = pars(Acts::ePHI); double theta = pars(Acts::eTHETA); momentum << p * sin(theta) * cos(phi), p * sin(theta) * sin(phi), p * cos(theta); return momentum; } static ParVector_t global2curvilinear(const ActsVectorD<3>&, const ActsVectorD<3>& mom, double charge) { ParVector_t parameters; parameters << 0, 0, mom.phi(), mom.theta(), ((std::abs(charge) < 1e-4) ? 1. : charge) / mom.mag(); return parameters; } // ... }; } // end of namespace detail } // end of namespace Acts The default parameterisation is chosen to be the ATLAS parameterisation: namespace Acts { enum ParDef : unsigned int { eLOC_0 = 0, ///< first coordinate in local surface frame eLOC_1 = 1, ///< second coordinate in local surface frame eLOC_R = eLOC_0, eLOC_PHI = eLOC_1, eLOC_RPHI = eLOC_0, eLOC_Z = eLOC_1, eLOC_X = eLOC_0, eLOC_Y = eLOC_1, eLOC_D0 = eLOC_0, eLOC_Z0 = eLOC_1, ePHI = 2, ///< phi direction of momentum in global frame eTHETA = 3, ///< theta direction of momentum in global frame eQOP = 4, ///< charge/momentum for charged tracks, for neutral tracks it is /// 1/momentum NGlobalPars }; Changing the default parameter definition and transformation needs recompilation of the Acts Software and redefinition of the relevant Plugin: target_compile_definitions (Acts::Core PUBLIC -DACTS_PARAMETER_DEFINITIONS_PLUGIN=\"${ACTS_PARAMETER_DEFINITIONS_PLUGIN}\")","title":"Track parameterisation"},{"location":"modules_eventdata/#neutral-and-charged-representations","text":"Track parameters come in a charged and neutral flavor, the flavor is defined using a template parameter, which either is a ChargedPolicy class for charged track parameter representation: namespace Acts { typedef SingleTrackParameters<ChargedPolicy> TrackParameters; typedef SingleCurvilinearTrackParameters<ChargedPolicy> CurvilinearParameters; typedef SingleBoundTrackParameters<ChargedPolicy> BoundParameters; } // end of namespace Acts Or, respectively, a NeutralPolicy object namespace Acts { typedef SingleTrackParameters<NeutralPolicy> NeutralParameters; typedef SingleCurvilinearTrackParameters<NeutralPolicy> NeutralCurvilinearParameters; typedef SingleBoundTrackParameters<NeutralPolicy> NeutralBoundParameters; } // end of namespace Acts","title":"Neutral and charged representations"},{"location":"modules_eventdata/#multivariant-representation","text":"Multivariant fitters, such as the Gaussian Sum filter rely on a multi-component description of the track, which requires the definition of multivariant track paramters. For Acts, as Propagator and Extrapolator are written for type templates, a multivariant definition of track parameterisation is planned in order to integrate directly with the core software.","title":"Multivariant representation"},{"location":"modules_eventdata/#measurements","text":"Measurements exist as uncalibrated and calibrated types. While uncalibrated measurements are the direct output of the data formation, during the track fit, when trajectory information (or information about the trajectory hypothesis) is available, certain calibration steps can help to improve the track fit quality.","title":"Measurements"},{"location":"modules_eventdata/#uncalibrated-measurements","text":"Measurements in a detector can be one to many-dimensional (covering the full range up to the full track parameterisation). template <typename Identifier, ParID_t... params> class Measurement { // check type conditions static_assert(std::is_copy_constructible<Identifier>::value, \"'Identifier' must be copy-constructible\"); static_assert(std::is_move_constructible<Identifier>::value, \"'Identifier' must be move-constructible\"); static_assert(std::is_copy_assignable<Identifier>::value, \"'Identifier' must be copy-assignable\"); static_assert(std::is_move_assignable<Identifier>::value, \"'Identifier' must be move-assignable\"); private: // private typedef's typedef ParameterSet<params...> ParSet_t; ///< type of the underlying ParameterSet object // ... }; In order to minimise the computational cost (and differently from the original ATLAS code base), the dimension of the measurement has to be fixed at compile time.","title":"Uncalibrated measurements"},{"location":"modules_eventdata/#calibrated-measurements","text":"Calibrated measurements are temporary objects needed for track fitting and belonging to a track. Still to be defined","title":"Calibrated measurements"},{"location":"modules_fitting/","text":"To be written","title":"Track fitting"},{"location":"modules_geometry/","text":"Geometry module Geometry module GeometryObject base class and GeometryID Surface classes Layer classes Volume classes Material description Geometry building Attaching a 3D detector geometry Existing Plugins for 3D geometry libraries Layer building Volume building, packing and gluing The Acts geometry model is strongly based on the ATLAS Tracking geometry. Its core is built on a surface-based description that make up all geometry objects of higher complexity. This design has been chosen as the surface objects can be used together with the track propagation module and thus all geometry objects become natively integrated into the tracking software. GeometryObject base class and GeometryID All geometry objects in Acts inherit from a virtual GeometryObject base class /// @class GeometryObject /// /// Base class to provide GeometryID interface: /// - simple set and get /// /// It also provides the binningPosition method for /// Geometry geometrical object to be binned in BinnedArrays /// class GeometryObject { public: /// default constructor GeometryObject() : m_geoID(0) {} /// constructor from a ready-made value /// /// @param geoID the geometry identifier of the object GeometryObject(const GeometryID& geoID) : m_geoID(geoID) {} /// assignment operator /// /// @param geoID the source geoID GeometryObject& operator=(const GeometryObject& geoID) { if (&geoID != this) { m_geoID = geoID.m_geoID; } return *this; } /// Return the value /// @return the geometry id by reference const GeometryID& geoID() const; /// Force a binning position method /// /// @param bValue is the value in which you want to bin /// /// @return vector 3D used for the binning schema virtual const Vector3D binningPosition(BinningValue bValue) const = 0; /// Implement the binningValue /// /// @param bValue is the dobule in which you want to bin /// /// @return float to be used for the binning schema double binningPositionValue(BinningValue bValue) const; /// Set the value /// /// @param geoID the geometry identifier to be assigned void assignGeoID(const GeometryID& geoID); protected: GeometryID m_geoID; }; This class ensures that a unique GeometryID is assigned to every geometry object. The GeometryID is mainly used for fast identification of the type of the geometry object (as most are either extensions or containers of the Surface objects) and for the identification of the geometry surfaces after building, e.g. for the uploading/assigning of material to the surface after creation. The GeometryID uses a simple masking procedure for applying an identification schema. It is used for Acts internal applications, such as material mapping, but not for EventData and Geometry identification in an experiment setup, for this the Identifier class is to be used and/or defined. typedef uint64_t geo_id_value; namespace Acts { /// @class GeometryID /// /// Identifier for Geometry nodes - packing the /// - (Sensitive) Surfaces - uses counting through sensitive surfaces /// - (Approach) Surfaces - uses counting approach surfaces /// - (Layer) Surfaces - uses counting confined layers /// - (Boundary) Surfaces - uses counting through boundary surfaces /// - Volumes - uses counting given by TrackingGeometry class GeometryID { public: const static geo_id_value volume_mask = 0xff00000000000000; const static geo_id_value boundary_mask = 0x00ff000000000000; const static geo_id_value layer_mask = 0x0000ff0000000000; const static geo_id_value approach_mask = 0x000000f000000000; const static geo_id_value sensitive_mask = 0x0000000ffff00000; const static geo_id_value channel_mask = 0x00000000000fffff; ... }; Surface classes The Surface class builds the core class of all geometry objects and can be used natively with the propagation and extrapolation modules. The common Surface virtual base defines the public interface of all surfaces. The different concrete Surface classes are defined by their respective native local coordinate system, while different shapes on surfaces are defined by SurfaceBounds classes which every surface must provide. In case of boundless surfaces, a special InfiniteBounds class is available. Surface Type Local Coordinates Bound Types available ConeSurface [rphi, z] ConeBounds CylinderSurface [r, phi] CylinderBounds DiscSurface [r, phi] RadialBounds , DiscTrapezoidalBounds PlaneSurface [x, y] RectangleBounds , TrapezoidalBounds , TriangleBounds , InfiniteBounds , EllipseBounds PerigeeSurface , StrawSurface [d, z] CylinderBounds Layer classes The Layer class is an extension of the Surface class that allows the definition of sub surfaces (sensitive surfaces for modules, or extra material surfaces). The Layer can simply correspond to a 'virtual' surface in the detector description or represent a more complex object that may contain: a representing surface, which is accessible via a representingSurface() method an array of contained surfaces, accessible via surfaceArray() method approach surfaces (i.e. boundary surface of the volume occupied by the layer) surface material description on any of the confined surfaces The following illustration shows an x-y view of a cylinder layer with planar detection modules: Modules can be sorted onto layer using all supported binning methods described through the SurfaceArray class, the binning can be adjusted to fit as good as possible. The un-occupied space in a volume which contains a layer array is filled with objects of type NavigationLayer , which allows that in a fully static geometry setp, every single point in a volume can be associated with a layer. Layer objects are confined together in a special LayerArray class and can be contained by a TrackingVolume . Volume classes The Volume class is a container of BoundarySurface objects, where each BoundarySurface is an extension of the Surface class with additional information abouit the attached Volumes. The normal vector of the surface defines an inside (opposite w.r.t. the normal vector) and an outside (along w.r.t. the normal vector) direction. Either a single volume or an array of volumes can be attached to a volume. The simples volume class is just a collection of surfaces, where the TrackingVolume describes a volume that can contain: an array of contained layers an array of contained volumes (as a container volume) an array of contained volumes (as floating objects) a volume based material description The shape of the volume is defined by VolumeBounds classes that create the corresponding bounding surfaces and register the attachment to the volume itself at creation. Material description Two types of material description exist, one for a surface based material, one for a volume based material. They will be dealt with differently in the extrapolation. The basic information for any material is: the radiation length X0 the nuclear interaction length L0 the atomic weight A the atomic charge Z the density of the material This information is confined together in the Material class. Surface based material extends this material information by representative thickness, the corresponding object is called MaterialProperties . The thickness hereby can be arbitrarily chosen in order to regulate the material budget, it does not have to represent the actual thickness of a detector element. To attach it to a surface, a dedicated SurfaceMaterial class (or it's extensions) is used, which allows to also describe binned material. Possible extensions are: HomogeneousSurfaceMaterial , homogeneous material description on a surface BinnedSurfaceMaterial , an arbitrarily binned material description with a corresponding BinUtility object ProtoSurfaceMaterial , only binning description (without material) to be used in the material mapping process Geometry building The geometry building procedure follows the ATLAS TrackingGeometry philosophy of a static frame of glued volumes, that lead the navigation flow through the geometry, Attaching a 3D detector geometry Usually, a 3D detector model geometry exists, which is either native to the full detector simulation (Geant4) or is translated into it. This model, however, is in general too detailed for track reconstruction: navigating through the detailed detector geometry For most part of the track reconstruction, only a surface based description of the detector is needed, in order to allow (surface based) material integration and parametrisation/prediction of trajectories on detection surfaces. It is thus necessary that the detection surfaces are described to full detail in the reconstruction geometry (called TrackingGeometry ). This is guaranteed by a proxy mechanism that connects the detection elements (conveniently called DetectorElement ) to Surface object in the reconstruction: Existing Plugins for 3D geometry libraries Very simple helper methods for 3D libraries exist, they are certainly not optimised, but used for templating: TGeoDetectorElement connects a TGeo volume to a Surface DD4HepDetectorElement connects a DD4Hep volume (based on TGeo) to a Surface Layer building Surface object that are to be grouped on a layer should be put into a SurfaceArray and provided to the layer. Certain helper tools exist to ease the translation and create appropriate binning structure: The SurfaceArrayCreator can create cylindrical, disc-like & planar layers, where the dimensions of the layer are determined by parsing the provided surfaces. Additionally, an envelope covering the surfaces can be chosen. Volume building, packing and gluing The philosophy of the TrackingGeometry is a fully connective geometry setup, i.e. TrackingVolume objects are either pure containers for other contained TrackingVolume instances (where the contained volumes fully fill the space of the container volume), or are fully attached via the boundary surface mechanism. The boundary surfaces then act as portals from one TrackingVolume into the next one along the trajectory. The process to create a fully connected tracking geometry is called glueing. Wherever possible, common boundary surfaces are shared , where this is not possible, they are attached . For cylindrical detector setups, a dedicated CylinderVolumeBuilder is provided, which performs a variety of volume building, packing and gluing.","title":"Geometry"},{"location":"modules_geometry/#geometry-module","text":"Geometry module GeometryObject base class and GeometryID Surface classes Layer classes Volume classes Material description Geometry building Attaching a 3D detector geometry Existing Plugins for 3D geometry libraries Layer building Volume building, packing and gluing The Acts geometry model is strongly based on the ATLAS Tracking geometry. Its core is built on a surface-based description that make up all geometry objects of higher complexity. This design has been chosen as the surface objects can be used together with the track propagation module and thus all geometry objects become natively integrated into the tracking software.","title":"Geometry module"},{"location":"modules_geometry/#geometryobject-base-class-and-geometryid","text":"All geometry objects in Acts inherit from a virtual GeometryObject base class /// @class GeometryObject /// /// Base class to provide GeometryID interface: /// - simple set and get /// /// It also provides the binningPosition method for /// Geometry geometrical object to be binned in BinnedArrays /// class GeometryObject { public: /// default constructor GeometryObject() : m_geoID(0) {} /// constructor from a ready-made value /// /// @param geoID the geometry identifier of the object GeometryObject(const GeometryID& geoID) : m_geoID(geoID) {} /// assignment operator /// /// @param geoID the source geoID GeometryObject& operator=(const GeometryObject& geoID) { if (&geoID != this) { m_geoID = geoID.m_geoID; } return *this; } /// Return the value /// @return the geometry id by reference const GeometryID& geoID() const; /// Force a binning position method /// /// @param bValue is the value in which you want to bin /// /// @return vector 3D used for the binning schema virtual const Vector3D binningPosition(BinningValue bValue) const = 0; /// Implement the binningValue /// /// @param bValue is the dobule in which you want to bin /// /// @return float to be used for the binning schema double binningPositionValue(BinningValue bValue) const; /// Set the value /// /// @param geoID the geometry identifier to be assigned void assignGeoID(const GeometryID& geoID); protected: GeometryID m_geoID; }; This class ensures that a unique GeometryID is assigned to every geometry object. The GeometryID is mainly used for fast identification of the type of the geometry object (as most are either extensions or containers of the Surface objects) and for the identification of the geometry surfaces after building, e.g. for the uploading/assigning of material to the surface after creation. The GeometryID uses a simple masking procedure for applying an identification schema. It is used for Acts internal applications, such as material mapping, but not for EventData and Geometry identification in an experiment setup, for this the Identifier class is to be used and/or defined. typedef uint64_t geo_id_value; namespace Acts { /// @class GeometryID /// /// Identifier for Geometry nodes - packing the /// - (Sensitive) Surfaces - uses counting through sensitive surfaces /// - (Approach) Surfaces - uses counting approach surfaces /// - (Layer) Surfaces - uses counting confined layers /// - (Boundary) Surfaces - uses counting through boundary surfaces /// - Volumes - uses counting given by TrackingGeometry class GeometryID { public: const static geo_id_value volume_mask = 0xff00000000000000; const static geo_id_value boundary_mask = 0x00ff000000000000; const static geo_id_value layer_mask = 0x0000ff0000000000; const static geo_id_value approach_mask = 0x000000f000000000; const static geo_id_value sensitive_mask = 0x0000000ffff00000; const static geo_id_value channel_mask = 0x00000000000fffff; ... };","title":"GeometryObject base class and GeometryID"},{"location":"modules_geometry/#surface-classes","text":"The Surface class builds the core class of all geometry objects and can be used natively with the propagation and extrapolation modules. The common Surface virtual base defines the public interface of all surfaces. The different concrete Surface classes are defined by their respective native local coordinate system, while different shapes on surfaces are defined by SurfaceBounds classes which every surface must provide. In case of boundless surfaces, a special InfiniteBounds class is available. Surface Type Local Coordinates Bound Types available ConeSurface [rphi, z] ConeBounds CylinderSurface [r, phi] CylinderBounds DiscSurface [r, phi] RadialBounds , DiscTrapezoidalBounds PlaneSurface [x, y] RectangleBounds , TrapezoidalBounds , TriangleBounds , InfiniteBounds , EllipseBounds PerigeeSurface , StrawSurface [d, z] CylinderBounds","title":"Surface classes"},{"location":"modules_geometry/#layer-classes","text":"The Layer class is an extension of the Surface class that allows the definition of sub surfaces (sensitive surfaces for modules, or extra material surfaces). The Layer can simply correspond to a 'virtual' surface in the detector description or represent a more complex object that may contain: a representing surface, which is accessible via a representingSurface() method an array of contained surfaces, accessible via surfaceArray() method approach surfaces (i.e. boundary surface of the volume occupied by the layer) surface material description on any of the confined surfaces The following illustration shows an x-y view of a cylinder layer with planar detection modules: Modules can be sorted onto layer using all supported binning methods described through the SurfaceArray class, the binning can be adjusted to fit as good as possible. The un-occupied space in a volume which contains a layer array is filled with objects of type NavigationLayer , which allows that in a fully static geometry setp, every single point in a volume can be associated with a layer. Layer objects are confined together in a special LayerArray class and can be contained by a TrackingVolume .","title":"Layer classes"},{"location":"modules_geometry/#volume-classes","text":"The Volume class is a container of BoundarySurface objects, where each BoundarySurface is an extension of the Surface class with additional information abouit the attached Volumes. The normal vector of the surface defines an inside (opposite w.r.t. the normal vector) and an outside (along w.r.t. the normal vector) direction. Either a single volume or an array of volumes can be attached to a volume. The simples volume class is just a collection of surfaces, where the TrackingVolume describes a volume that can contain: an array of contained layers an array of contained volumes (as a container volume) an array of contained volumes (as floating objects) a volume based material description The shape of the volume is defined by VolumeBounds classes that create the corresponding bounding surfaces and register the attachment to the volume itself at creation.","title":"Volume classes"},{"location":"modules_geometry/#material-description","text":"Two types of material description exist, one for a surface based material, one for a volume based material. They will be dealt with differently in the extrapolation. The basic information for any material is: the radiation length X0 the nuclear interaction length L0 the atomic weight A the atomic charge Z the density of the material This information is confined together in the Material class. Surface based material extends this material information by representative thickness, the corresponding object is called MaterialProperties . The thickness hereby can be arbitrarily chosen in order to regulate the material budget, it does not have to represent the actual thickness of a detector element. To attach it to a surface, a dedicated SurfaceMaterial class (or it's extensions) is used, which allows to also describe binned material. Possible extensions are: HomogeneousSurfaceMaterial , homogeneous material description on a surface BinnedSurfaceMaterial , an arbitrarily binned material description with a corresponding BinUtility object ProtoSurfaceMaterial , only binning description (without material) to be used in the material mapping process","title":"Material description"},{"location":"modules_geometry/#geometry-building","text":"The geometry building procedure follows the ATLAS TrackingGeometry philosophy of a static frame of glued volumes, that lead the navigation flow through the geometry,","title":"Geometry building"},{"location":"modules_geometry/#attaching-a-3d-detector-geometry","text":"Usually, a 3D detector model geometry exists, which is either native to the full detector simulation (Geant4) or is translated into it. This model, however, is in general too detailed for track reconstruction: navigating through the detailed detector geometry For most part of the track reconstruction, only a surface based description of the detector is needed, in order to allow (surface based) material integration and parametrisation/prediction of trajectories on detection surfaces. It is thus necessary that the detection surfaces are described to full detail in the reconstruction geometry (called TrackingGeometry ). This is guaranteed by a proxy mechanism that connects the detection elements (conveniently called DetectorElement ) to Surface object in the reconstruction:","title":"Attaching a 3D detector geometry"},{"location":"modules_geometry/#existing-plugins-for-3d-geometry-libraries","text":"Very simple helper methods for 3D libraries exist, they are certainly not optimised, but used for templating: TGeoDetectorElement connects a TGeo volume to a Surface DD4HepDetectorElement connects a DD4Hep volume (based on TGeo) to a Surface","title":"Existing Plugins for 3D geometry libraries"},{"location":"modules_geometry/#layer-building","text":"Surface object that are to be grouped on a layer should be put into a SurfaceArray and provided to the layer. Certain helper tools exist to ease the translation and create appropriate binning structure: The SurfaceArrayCreator can create cylindrical, disc-like & planar layers, where the dimensions of the layer are determined by parsing the provided surfaces. Additionally, an envelope covering the surfaces can be chosen.","title":"Layer building"},{"location":"modules_geometry/#volume-building-packing-and-gluing","text":"The philosophy of the TrackingGeometry is a fully connective geometry setup, i.e. TrackingVolume objects are either pure containers for other contained TrackingVolume instances (where the contained volumes fully fill the space of the container volume), or are fully attached via the boundary surface mechanism. The boundary surfaces then act as portals from one TrackingVolume into the next one along the trajectory. The process to create a fully connected tracking geometry is called glueing. Wherever possible, common boundary surfaces are shared , where this is not possible, they are attached . For cylindrical detector setups, a dedicated CylinderVolumeBuilder is provided, which performs a variety of volume building, packing and gluing.","title":"Volume building, packing and gluing"},{"location":"modules_pattern/","text":"To be written","title":"Pattern Recognition"},{"location":"modules_propagation/","text":"Propagation and Extrapolation The track propagation is an essential part of track reconstruction. The Acts propgation code is based on the ATLAS RungeKuttaPropagator , against which newere developments where validated. It has since been removed from the codebase. Steppers and Propagator The Acts propagator allows for different Stepper implementations provided as a class template. Following the general Acts design, each stepper has a nested cache struct, which is used for caching the field cell and the update the jacobian for covariance propagation. AtlasStepper The AtlasStepper is a pure transcript from the ATLAS RungeKuttaPropagator and RungeKuttaUtils , and operators with an internal structure of double[] . This example shows a code snippet from the numerical integration. while (h != 0.) { double S3 = (1. / 3.) * h, S4 = .25 * h, PS2 = Pi * h; // First point // double H0[3] = {f0[0] * PS2, f0[1] * PS2, f0[2] * PS2}; double A0 = A[1] * H0[2] - A[2] * H0[1]; double B0 = A[2] * H0[0] - A[0] * H0[2]; double C0 = A[0] * H0[1] - A[1] * H0[0]; double A2 = A0 + A[0]; double B2 = B0 + A[1]; double C2 = C0 + A[2]; double A1 = A2 + A[0]; double B1 = B2 + A[1]; double C1 = C2 + A[2]; // Second point // if (!Helix) { const Vector3D pos(R[0] + A1 * S4, R[1] + B1 * S4, R[2] + C1 * S4); f = getField(cache, pos); } else { f = f0; } double H1[3] = {f[0] * PS2, f[1] * PS2, f[2] * PS2}; double A3 = (A[0] + B2 * H1[2]) - C2 * H1[1]; double B3 = (A[1] + C2 * H1[0]) - A2 * H1[2]; double C3 = (A[2] + A2 * H1[1]) - B2 * H1[0]; double A4 = (A[0] + B3 * H1[2]) - C3 * H1[1]; double B4 = (A[1] + C3 * H1[0]) - A3 * H1[2]; double C4 = (A[2] + A3 * H1[1]) - B3 * H1[0]; double A5 = 2. * A4 - A[0]; double B5 = 2. * B4 - A[1]; double C5 = 2. * C4 - A[2]; // Last point // if (!Helix) { const Vector3D pos(R[0] + h * A4, R[1] + h * B4, R[2] + h * C4); f = getField(cache, pos); } else { f = f0; } double H2[3] = {f[0] * PS2, f[1] * PS2, f[2] * PS2}; double A6 = B5 * H2[2] - C5 * H2[1]; double B6 = C5 * H2[0] - A5 * H2[2]; double C6 = A5 * H2[1] - B5 * H2[0]; } EigenStepper The EigenStepper implements the same functionality as the ATLAS stepper, however, the stepping code is rewritten with using Eigen primitives. The following code snippet shows the Runge-Kutta stepping code. // The following functor starts to perform a Runge-Kutta step of a certain // size, going up to the point where it can return an estimate of the local // integration error. The results are stated in the local variables above, // allowing integration to continue once the error is deemed satisfactory const auto tryRungeKuttaStep = [&](const double h) -> bool { // State the square and half of the step size h2 = h * h; half_h = h * 0.5; // Second Runge-Kutta point const Vector3D pos1 = state.stepping.pos + half_h * state.stepping.dir + h2 * 0.125 * sd.k1; sd.B_middle = getField(state.stepping, pos1); if (!state.stepping.extension.k2( state, sd.k2, sd.B_middle, half_h, sd.k1)) { return false; } // Third Runge-Kutta point if (!state.stepping.extension.k3( state, sd.k3, sd.B_middle, half_h, sd.k2)) { return false; } // Last Runge-Kutta point const Vector3D pos2 = state.stepping.pos + h * state.stepping.dir + h2 * 0.5 * sd.k3; sd.B_last = getField(state.stepping, pos2); if (!state.stepping.extension.k4(state, sd.k4, sd.B_last, h, sd.k3)) { return false; } // Return an estimate of the local integration error error_estimate = std::max( h2 * (sd.k1 - sd.k2 - sd.k3 + sd.k4).template lpNorm<1>(), 1e-20); return true; }; The code includes the extension mechanism, which allows extenting the numerical integration. This is implemented for the custom logic required to integrate through a volume with dense material.","title":"Propagation"},{"location":"modules_propagation/#propagation-and-extrapolation","text":"The track propagation is an essential part of track reconstruction. The Acts propgation code is based on the ATLAS RungeKuttaPropagator , against which newere developments where validated. It has since been removed from the codebase.","title":"Propagation and Extrapolation"},{"location":"modules_propagation/#steppers-and-propagator","text":"The Acts propagator allows for different Stepper implementations provided as a class template. Following the general Acts design, each stepper has a nested cache struct, which is used for caching the field cell and the update the jacobian for covariance propagation.","title":"Steppers and Propagator"},{"location":"modules_propagation/#atlasstepper","text":"The AtlasStepper is a pure transcript from the ATLAS RungeKuttaPropagator and RungeKuttaUtils , and operators with an internal structure of double[] . This example shows a code snippet from the numerical integration. while (h != 0.) { double S3 = (1. / 3.) * h, S4 = .25 * h, PS2 = Pi * h; // First point // double H0[3] = {f0[0] * PS2, f0[1] * PS2, f0[2] * PS2}; double A0 = A[1] * H0[2] - A[2] * H0[1]; double B0 = A[2] * H0[0] - A[0] * H0[2]; double C0 = A[0] * H0[1] - A[1] * H0[0]; double A2 = A0 + A[0]; double B2 = B0 + A[1]; double C2 = C0 + A[2]; double A1 = A2 + A[0]; double B1 = B2 + A[1]; double C1 = C2 + A[2]; // Second point // if (!Helix) { const Vector3D pos(R[0] + A1 * S4, R[1] + B1 * S4, R[2] + C1 * S4); f = getField(cache, pos); } else { f = f0; } double H1[3] = {f[0] * PS2, f[1] * PS2, f[2] * PS2}; double A3 = (A[0] + B2 * H1[2]) - C2 * H1[1]; double B3 = (A[1] + C2 * H1[0]) - A2 * H1[2]; double C3 = (A[2] + A2 * H1[1]) - B2 * H1[0]; double A4 = (A[0] + B3 * H1[2]) - C3 * H1[1]; double B4 = (A[1] + C3 * H1[0]) - A3 * H1[2]; double C4 = (A[2] + A3 * H1[1]) - B3 * H1[0]; double A5 = 2. * A4 - A[0]; double B5 = 2. * B4 - A[1]; double C5 = 2. * C4 - A[2]; // Last point // if (!Helix) { const Vector3D pos(R[0] + h * A4, R[1] + h * B4, R[2] + h * C4); f = getField(cache, pos); } else { f = f0; } double H2[3] = {f[0] * PS2, f[1] * PS2, f[2] * PS2}; double A6 = B5 * H2[2] - C5 * H2[1]; double B6 = C5 * H2[0] - A5 * H2[2]; double C6 = A5 * H2[1] - B5 * H2[0]; }","title":"AtlasStepper"},{"location":"modules_propagation/#eigenstepper","text":"The EigenStepper implements the same functionality as the ATLAS stepper, however, the stepping code is rewritten with using Eigen primitives. The following code snippet shows the Runge-Kutta stepping code. // The following functor starts to perform a Runge-Kutta step of a certain // size, going up to the point where it can return an estimate of the local // integration error. The results are stated in the local variables above, // allowing integration to continue once the error is deemed satisfactory const auto tryRungeKuttaStep = [&](const double h) -> bool { // State the square and half of the step size h2 = h * h; half_h = h * 0.5; // Second Runge-Kutta point const Vector3D pos1 = state.stepping.pos + half_h * state.stepping.dir + h2 * 0.125 * sd.k1; sd.B_middle = getField(state.stepping, pos1); if (!state.stepping.extension.k2( state, sd.k2, sd.B_middle, half_h, sd.k1)) { return false; } // Third Runge-Kutta point if (!state.stepping.extension.k3( state, sd.k3, sd.B_middle, half_h, sd.k2)) { return false; } // Last Runge-Kutta point const Vector3D pos2 = state.stepping.pos + h * state.stepping.dir + h2 * 0.5 * sd.k3; sd.B_last = getField(state.stepping, pos2); if (!state.stepping.extension.k4(state, sd.k4, sd.B_last, h, sd.k3)) { return false; } // Return an estimate of the local integration error error_estimate = std::max( h2 * (sd.k1 - sd.k2 - sd.k3 + sd.k4).template lpNorm<1>(), 1e-20); return true; }; The code includes the extension mechanism, which allows extenting the numerical integration. This is implemented for the custom logic required to integrate through a volume with dense material.","title":"EigenStepper"}]}